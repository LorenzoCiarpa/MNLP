{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNtbaQHcWDz5"
      },
      "source": [
        "# **Natural Language Processing Homework 1: Event Detection (ED)**\n",
        "\n",
        "*Authors:*\n",
        "\n",
        "*   Lorenzo Ciarpaglini (student ID: 1813738)\n",
        "\n",
        "This is the notebook running my implementation of the ED task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5TXd1-PmeIF"
      },
      "source": [
        "# Preliminary\n",
        "\n",
        "Run this section to import and/or download the libraries needed for the proper functioning of the notebook and to download the dataset from source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-7FFAHRd2xz",
        "outputId": "64091049-3228-4469-f1ad-4e46bd5acb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp2023-hw1'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 30 (delta 3), reused 3 (delta 3), pack-reused 21\u001b[K\n",
            "Receiving objects: 100% (30/30), 1.95 MiB | 20.76 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "! rm -rf sample_data/\n",
        "! git clone https://github.com/SapienzaNLP/nlp2023-hw1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UuoWrolpjZ6"
      },
      "outputs": [],
      "source": [
        "#import libraries here\n",
        "import os\n",
        "import copy\n",
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List #, Union, Set, Callable\n",
        "from xml.dom import minidom\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import json\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "\n",
        "SEED:int = 1234"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7EXkJ94Orgf"
      },
      "outputs": [],
      "source": [
        "#some global parameters & constants\n",
        "DATASET_DIR = \"nlp2023-hw1/data\"\n",
        "PRINT_BAR = '-' * 10\n",
        "\n",
        "file_type_dir = \"jsonl\"  #or \"xml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKBqcZTiVSmn"
      },
      "source": [
        "# Dataset playground\n",
        "\n",
        "This section contains code to inspect the dataset and comments to intepret its main features. Many of the following methods will be imported in what will be the final `Dataset` class. Open this section for further explanations on the strategies adopted for building the final version of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlQv_0UTI42n"
      },
      "outputs": [],
      "source": [
        "def downloadDataset(dataset_prefix):\n",
        "  data_path = os.path.join(DATASET_DIR, dataset_prefix)\n",
        "  data_path += '.' + file_type_dir\n",
        "\n",
        "  with open(data_path) as f:\n",
        "    sentences = f.read().splitlines()\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "      sentences[i] = json.loads(sentence)\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQPhzpzgJGwK"
      },
      "outputs": [],
      "source": [
        "train_set = downloadDataset('train')\n",
        "test_set = downloadDataset('test')\n",
        "dev_set = downloadDataset('dev')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS1Od-5j-TCA",
        "outputId": "c09ed104-4158-4e01-c7a8-41d121652ca6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'idx': 0,\n",
              " 'tokens': ['However',\n",
              "  ',',\n",
              "  'as',\n",
              "  'the',\n",
              "  'day',\n",
              "  'progressed',\n",
              "  ',',\n",
              "  'Morris',\n",
              "  'improved',\n",
              "  'while',\n",
              "  'Park',\n",
              "  'fell',\n",
              "  'away',\n",
              "  '.'],\n",
              " 'labels': ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O']}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dev_set[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPhojnAmgek5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OgLSPvVGF70"
      },
      "source": [
        "# From dataset to dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVcIb7dwJeo9"
      },
      "outputs": [],
      "source": [
        "def createVocabolary(data_set):\n",
        "  vocab = {}\n",
        "  tags = {}\n",
        "  # words_set = set()\n",
        "  words_set = []\n",
        "  tags_set = []\n",
        "\n",
        "  for data in data_set:\n",
        "    # data = json.loads(data)\n",
        "\n",
        "    for token in data['tokens']:\n",
        "      # words_set.add(token)\n",
        "      if token not in words_set:\n",
        "        words_set.append(token)\n",
        "\n",
        "    for label in data['labels']:\n",
        "      # words_set.add(token)\n",
        "      if label not in tags_set:\n",
        "        tags_set.append(label)\n",
        "\n",
        "  for idx, word in enumerate(words_set):\n",
        "    vocab[word] = idx\n",
        "\n",
        "  vocab['UNK'] = idx + 1\n",
        "  vocab['PAD'] = idx + 2\n",
        "\n",
        "  for idx, tag in enumerate(tags_set):\n",
        "    tags[tag] = idx\n",
        "\n",
        "  return vocab, tags\n",
        "\n",
        "def createWordsFile(data_set):\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fQlD4qJ7bDr"
      },
      "outputs": [],
      "source": [
        "def map_to_dict(data_set, vocab, tags):\n",
        "  data_set_new = []\n",
        "\n",
        "  for i, data in enumerate(data_set):\n",
        "    data_set_new.append(data.copy())\n",
        "\n",
        "    data_set_new[i]['tokens_mapped'] = []\n",
        "    data_set_new[i]['labels_mapped'] = []\n",
        "\n",
        "    for token in data['tokens']:\n",
        "      data_set_new[i]['tokens_mapped'].append(vocab[token] if token in vocab else vocab['UNK'])\n",
        "\n",
        "    for label in data['labels']:\n",
        "      data_set_new[i]['labels_mapped'].append(tags[label])\n",
        "\n",
        "    data_set_new[i]['len'] = len(data['tokens'])\n",
        "\n",
        "  return data_set_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am66RoXn_NuC"
      },
      "outputs": [],
      "source": [
        "def get_max_len(data_set):\n",
        "  longest = 0\n",
        "  for i, elem in enumerate(data_set):\n",
        "    if elem['len'] > longest:\n",
        "      longest = elem['len']\n",
        "  return longest\n",
        "\n",
        "\n",
        "\n",
        "def mask_to_tensor(len_list, batch_size):\n",
        "    token_len = get_max_len(len_list)\n",
        "    tokens = torch.LongTensor(token_len, batch_size).fill_(0)\n",
        "    for i, s in enumerate(len_list):\n",
        "      tokens[:s['len'], i] = 1\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVy50riTHpPT"
      },
      "outputs": [],
      "source": [
        "def generate_batches(data_set):\n",
        "  batch = []\n",
        "\n",
        "  batches_tokens = []\n",
        "  batches_labels = []\n",
        "\n",
        "  max_len = get_max_len(data_set)\n",
        "  for i, data in enumerate(data_set):\n",
        "    batch.append(copy.deepcopy(data_set[i]))\n",
        "    # data_set[i]['tokens'].extend('PAD' for i in range(max_len - data_set[i]['len'] ))\n",
        "    batch[i]['tokens_mapped'].extend(vocab['PAD'] for i in range(max_len - data_set[i]['len']))\n",
        "    # data_set[i]['labels'].extend('PAD' for i in range(max_len - data_set[i]['len'] ))\n",
        "    batch[i]['labels_mapped'].extend(-1 for i in range(max_len - data_set[i]['len']))\n",
        "\n",
        "    batches_tokens.append(torch.LongTensor(batch[i]['tokens_mapped']))\n",
        "    batches_labels.append(torch.LongTensor(batch[i]['labels_mapped']))\n",
        "\n",
        "  batches_tokens = torch.stack(batches_tokens)\n",
        "  batches_labels = torch.stack(batches_labels)\n",
        "\n",
        "  return batches_tokens, batches_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpjqlQT7M8Ss"
      },
      "outputs": [],
      "source": [
        "#REMEMBER we have to add UNK('O' a label??), PAD(-1 as label)\n",
        "vocab, tags = createVocabolary(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jsGL4RX91Na"
      },
      "outputs": [],
      "source": [
        "train_set_mapped = map_to_dict(train_set, vocab, tags)\n",
        "test_set_mapped = map_to_dict(test_set, vocab, tags)\n",
        "dev_set_mapped = map_to_dict(dev_set, vocab, tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BGLeqKO-g7h"
      },
      "outputs": [],
      "source": [
        "print(len(vocab))\n",
        "print(len(tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0gjCvJ0zHRd"
      },
      "source": [
        "# Hyper Parameteres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNxemck7uW5K"
      },
      "outputs": [],
      "source": [
        "#hyper parameters\n",
        "hypers = {\n",
        "\n",
        "    'vocab_size': len(vocab),\n",
        "\n",
        "    'embedding_dim': 128,\n",
        "\n",
        "    'lstm_hidden_dim': 128,\n",
        "\n",
        "    'number_of_tags': len(tags),\n",
        "\n",
        "    'input_size': 768,\n",
        "\n",
        "    'hidden_size': 0,\n",
        "\n",
        "    'num_classes': 2,\n",
        "\n",
        "    'learning_rate': 2e-5,\n",
        "\n",
        "    'batch_size': 16,\n",
        "\n",
        "    'epochs': 1,\n",
        "\n",
        "    'dropout_rate': 0.2,\n",
        "\n",
        "    'print_step': 10,\n",
        "\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "}\n",
        "\n",
        "print(hypers['device'])\n",
        "\n",
        "class Dict2Class(object):\n",
        "\n",
        "    def __init__(self, my_dict):\n",
        "\n",
        "        for key in my_dict:\n",
        "            setattr(self, key, my_dict[key])\n",
        "\n",
        "hypers = Dict2Class(hypers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPePshLRuW5H"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHaDaSQquW5K"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hypers):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(hypers.vocab_size + 1, hypers.embedding_dim)\n",
        "\n",
        "        #the LSTM takens embedded sentence\n",
        "        self.lstm = nn.LSTM(hypers.embedding_dim, hypers.lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        #fc layer transforms the output to give the final output layer\n",
        "        self.fc = nn.Linear(hypers.lstm_hidden_dim * 2, hypers.number_of_tags)\n",
        "\n",
        "        self.dropout = nn.Dropout(hypers.dropout_rate)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    # def hidden_init(batch_size, hidden_size):\n",
        "    #   h0 = torch.zeros(batch_size, hidden_size).requires_grad_(False).to(hypers.device)\n",
        "    #   return h0\n",
        "\n",
        "    def forward(self, s):\n",
        "\n",
        "        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n",
        "\n",
        "        #run the LSTM along the sentences of length batch_max_len\n",
        "        s, _ = self.lstm(s)     # dim: batch_size x batch_max_len x lstm_hidden_dim\n",
        "\n",
        "        #reshape the Variable so that each row contains one token\n",
        "        s = s.reshape(-1, s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
        "\n",
        "        #apply the fully connected layer and obtain the output for each token\n",
        "        s = self.fc(s)          # dim: batch_size*batch_max_len x num_tags\n",
        "        s = self.dropout(s)\n",
        "\n",
        "        return F.log_softmax(s, dim=1)   #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-YdICDhXD12"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, labels):\n",
        "    #reshape labels to give a flat vector of length batch_size*seq_len\n",
        "    labels = labels.view(-1)\n",
        "\n",
        "    #mask out 'PAD' tokens\n",
        "    mask = (labels >= 0).float()\n",
        "\n",
        "    #the number of tokens is the sum of elements in mask\n",
        "    num_tokens = int(torch.sum(mask).item())\n",
        "\n",
        "    #pick the values corresponding to labels and multiply by mask\n",
        "    outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
        "\n",
        "    #cross entropy loss for all non 'PAD' tokens\n",
        "    return -torch.sum(outputs)/num_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY1hbKDRS-uR"
      },
      "source": [
        "# Instantion and Training Lightning\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgqT-uxfXITg",
        "outputId": "4dcc1515-5d4a-4556-ff40-2814ded34298"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (embedding): Embedding(38009, 128)\n",
              "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=11, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model = Net(hypers)\n",
        "model.to(hypers.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ3IuUaUXqsT"
      },
      "outputs": [],
      "source": [
        "# output = model(batches_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8hC8mZdYANj"
      },
      "outputs": [],
      "source": [
        "# loss_fn(output, batches_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgBqaqG-nzCf"
      },
      "outputs": [],
      "source": [
        "# accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hypers.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pAPaTs_nzCf"
      },
      "outputs": [],
      "source": [
        "\n",
        "losses = []\n",
        "accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtPeX_hgnzCf"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "for epoch in range(100):\n",
        "\n",
        "    num_iterations = len(train_set_mapped) // 16\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        batches_tokens, batches_labels = generate_batches(train_set_mapped[i * 16:(i+1)*16])\n",
        "\n",
        "        batches_tokens = batches_tokens.to(hypers.device)\n",
        "        batches_labels = batches_labels.to(hypers.device)\n",
        "\n",
        "        batch_size = batches_tokens.size(0)\n",
        "\n",
        "        output = model(batches_tokens)\n",
        "\n",
        "        loss = loss_fn(output, batches_labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss)\n",
        "        # accuracies.append(accuracy(output, labels).item())\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "      print(\"epoch: \", epoch, \" loss: \", loss.item())\n",
        "\n",
        "\n",
        "\n",
        "print(\"End\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-CKlhNCvFFj"
      },
      "outputs": [],
      "source": [
        "batches_tokens, batches_labels = generate_batches(dev_set_mapped[0:8])\n",
        "\n",
        "batches_tokens = batches_tokens.to(hypers.device)\n",
        "batches_labels = batches_labels.to(hypers.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWhsuHCLvucF"
      },
      "outputs": [],
      "source": [
        "batches_labels = batches_labels.reshape(-1)\n",
        "print(batches_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5e134r5v8c2"
      },
      "outputs": [],
      "source": [
        "mask = (batches_labels >= 0).float()\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9sS94UXwExk",
        "outputId": "1e239309-dfd2-4fee-acf3-62856891c40b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "190\n"
          ]
        }
      ],
      "source": [
        "num_tokens = int(torch.sum(mask).item())\n",
        "print(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKYtwSW3u5hU"
      },
      "outputs": [],
      "source": [
        "output = model(batches_tokens)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytn50eeRwoAM"
      },
      "outputs": [],
      "source": [
        "list(range(output.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCsLDEMlwtha"
      },
      "outputs": [],
      "source": [
        "output[list(range(output.shape[0])), batches_labels]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaf7AX92vOBo"
      },
      "outputs": [],
      "source": [
        "output[range(output.shape[0]), batches_labels].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72Fww0mcxwzd"
      },
      "outputs": [],
      "source": [
        "out = output[list(range(output.shape[0])), batches_labels] * mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "685TaIYI2HeN"
      },
      "outputs": [],
      "source": [
        "out.exp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrcR9O01GQdJ"
      },
      "outputs": [],
      "source": [
        "losses_pred = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQPYdazK9uML",
        "outputId": "60cc3f96-60ab-4947-a822-7a8f5c983995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  99  loss:  0.1873970776796341\n",
            "End\n"
          ]
        }
      ],
      "source": [
        "losses_pred = []\n",
        "\n",
        "model.eval()\n",
        "num_iterations = len(dev_set_mapped) // 16\n",
        "\n",
        "for i in range(num_iterations):\n",
        "\n",
        "    batches_tokens, batches_labels = generate_batches(dev_set_mapped[i * 16:(i+1)*16])\n",
        "\n",
        "    batches_tokens = batches_tokens.to(hypers.device)\n",
        "    batches_labels = batches_labels.to(hypers.device)\n",
        "\n",
        "    batch_size = batches_tokens.size(0)\n",
        "\n",
        "    output = model(batches_tokens)\n",
        "\n",
        "    loss = loss_fn(output, batches_labels)\n",
        "\n",
        "    # optimizer.zero_grad()\n",
        "    # # loss.backward()\n",
        "    # optimizer.step()\n",
        "\n",
        "    losses_pred.append(loss.detach().item())\n",
        "    # accuracies.append(accuracy(output, labels).item())\n",
        "\n",
        "if epoch % 1 == 0:\n",
        "  print(\"epoch: \", epoch, \" loss: \", loss.item())\n",
        "\n",
        "\n",
        "\n",
        "print(\"End\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjXgXzEQGf99",
        "outputId": "f16743c2-fef1-45d4-f294-2f8a1f4f164d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1858)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "torch.tensor(losses_pred).mean()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jNtbaQHcWDz5",
        "j5TXd1-PmeIF",
        "eKBqcZTiVSmn",
        "r1P5wnbOUgcx",
        "3OgLSPvVGF70",
        "EsjwbmXoBgso",
        "X0gjCvJ0zHRd",
        "OPePshLRuW5H",
        "oY1hbKDRS-uR",
        "sLXpfz3utYFS"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}