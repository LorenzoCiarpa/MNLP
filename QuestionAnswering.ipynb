{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UTZ3Fh1qZ_qp",
        "ksMe7XmpBv6q",
        "re7sDbyQDpY7",
        "6QXGnSWMDsoa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "UTZ3Fh1qZ_qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install -U sentence-transformers\n",
        "!pip install rake_nltk\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import gensim\n",
        "import math\n",
        "import string\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#import wget\n",
        "import spacy\n",
        "import scipy.stats\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "from rake_nltk import Rake\n",
        "import gensim.downloader as api\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words=stop_words.copy()\n",
        "stop_words.remove('no')\n",
        "stop_words.remove('not')\n",
        "\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "qa_model = pipeline(\"question-answering\")"
      ],
      "metadata": {
        "id": "-nj53ePLZ_qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1VPzI4vDoANpTmb0NnfoxJjM_zChfKFuP #class 5\n",
        "!gdown 1pHAQ3mbL1yOps2-Nzfzs891G4wtOXoKQ #class 6"
      ],
      "metadata": {
        "id": "MkHF_9hUZ_qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32add1eb-2a3b-4ac8-e276-0c9b8bda1a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VPzI4vDoANpTmb0NnfoxJjM_zChfKFuP\n",
            "To: /content/class_05.clean.txt\n",
            "100% 503k/503k [00:00<00:00, 5.94MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pHAQ3mbL1yOps2-Nzfzs891G4wtOXoKQ\n",
            "To: /content/class_06.clean.txt\n",
            "100% 1.32M/1.32M [00:00<00:00, 10.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PATH\n",
        "animal_path='/content/class_05.clean.txt'\n",
        "artifact_path= '/content/class_06.clean.txt'"
      ],
      "metadata": {
        "id": "d0vBsZvsZ_qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download dataset**"
      ],
      "metadata": {
        "id": "GI_CoLCjbQLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ANIMALS\n",
        "num_questions = 44\n",
        "num_items = 93\n",
        "\n",
        "path=animal_path\n",
        "is_animals = True\n",
        "\n",
        "file = open(path, \"r\")\n",
        "triples = [line.split('\\t') for line in file.read().splitlines()]\n"
      ],
      "metadata": {
        "id": "h2_K2b9Bz1VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ARTIFACTS\n",
        "num_questions = 55\n",
        "num_items = 134\n",
        "\n",
        "path=artifact_path\n",
        "is_animals = False\n",
        "\n",
        "file = open(path, \"r\")\n",
        "triples = [line.split('\\t') for line in file.read().splitlines()]\n"
      ],
      "metadata": {
        "id": "e2VfNxFU-UGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# items = []\n",
        "# questions = []\n",
        "# answers = []\n",
        "\n",
        "# corpus = {}\n",
        "\n",
        "# for i, triple in enumerate(triples):\n",
        "#   item = triple[0]\n",
        "#   question = triple[1]\n",
        "#   answer = triple[2]\n",
        "\n",
        "#   if item not in corpus:\n",
        "#     items.append(item)\n",
        "#     corpus[item] = {\n",
        "#         'questions': [],\n",
        "#         'answers': [],\n",
        "#     }\n",
        "\n",
        "#   corpus[item]['questions'].append(question)\n",
        "#   corpus[item]['answers'].append(answer)\n",
        "\n",
        "# for i, item in enumerate(items):\n",
        "#   questions.append(corpus[item]['questions'])\n",
        "#   answers.append(corpus[item]['questions'])\n"
      ],
      "metadata": {
        "id": "tmpqu8m7IaYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items = []\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for i,sentence in enumerate(triples):\n",
        "\n",
        "  if i % num_questions == 0:\n",
        "    items.append(sentence[0])\n",
        "    quest=[] #questions\n",
        "    ans=[] #answers\n",
        "\n",
        "  quest.append(sentence[1])\n",
        "  ans.append(sentence[2])\n",
        "\n",
        "  if i % num_questions == (num_questions-1):\n",
        "    questions.append(quest)\n",
        "    answers.append(ans)"
      ],
      "metadata": {
        "id": "3vXYKTZ3gdVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract keywords Functions**"
      ],
      "metadata": {
        "id": "5rSRVkBxGifb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(sentence, stopwords, lemmatize=True):\n",
        "  doc = spacy_nlp(sentence)\n",
        "  tokens = []\n",
        "\n",
        "  for token in doc:\n",
        "    token_text = token.lemma_ if lemmatize else token.text\n",
        "    token_text = token_text.lower()\n",
        "\n",
        "    if token_text in stopwords or token_text in string.punctuation:\n",
        "      continue\n",
        "\n",
        "    tokens.append(token_text)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "jDQyYPA8Z_qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_best_similarity(item, question, answer, model, threshold=0.5):\n",
        "\n",
        "  filtered_answer= preprocess_text(answer, stop_words, True)\n",
        "  filtered_question= preprocess_text(question, stop_words, True)\n",
        "\n",
        "  if filtered_answer[0]=='yes' or filtered_answer[0]=='no':\n",
        "    return [filtered_answer[0]]\n",
        "\n",
        "  if item in  model:\n",
        "    final_answer=[]\n",
        "    average_score=[]\n",
        "\n",
        "    for i,token in enumerate(filtered_answer):\n",
        "\n",
        "      if token not in model or token==item: # always discard item in answer\n",
        "        average_score.append(0)\n",
        "        continue\n",
        "\n",
        "      if token.replace('.', '').isnumeric(): #always keep numbers\n",
        "        average_score.append(1)\n",
        "        continue\n",
        "\n",
        "      similarities_answer_question = []\n",
        "\n",
        "      for j, token_question in enumerate(filtered_question):\n",
        "          if token_question in model and token in model:\n",
        "\n",
        "            sim = model.similarity(token, token_question)\n",
        "            similarities_answer_question.append(sim)\n",
        "\n",
        "      avg = sum(similarities_answer_question) / len(similarities_answer_question)\n",
        "      average_score.append(avg)\n",
        "\n",
        "\n",
        "    avg_score=np.array(average_score)\n",
        "    idx_avg = np.where(avg_score>threshold)[0]\n",
        "\n",
        "    if idx_avg.shape[0]>0:\n",
        "      for elem in idx_avg:\n",
        "        final_answer.append(filtered_answer[elem])\n",
        "      return final_answer\n",
        "\n",
        "    else: return [filtered_answer[np.array(average_score).argmax()]]\n",
        "\n",
        "  return filtered_answer"
      ],
      "metadata": {
        "id": "hEo5LqhzJipF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get elements with yes and no\n",
        "\n",
        "def extract_yes_no_answers(questions, answers, is_animals = False):\n",
        "  yes_no_final_answers = []\n",
        "\n",
        "  for i, item in enumerate(items):\n",
        "      item_answers = []\n",
        "\n",
        "      for j in range(num_questions):\n",
        "          all_answer = []\n",
        "\n",
        "          filtered_answer = preprocess_text(answers[i][j], stop_words, True)\n",
        "\n",
        "          if filtered_answer[0]=='yes' or filtered_answer[0]=='no':\n",
        "            item_answers.append(filtered_answer[0])\n",
        "\n",
        "          elif 'not' in filtered_answer:\n",
        "            item_answers.append('no')\n",
        "\n",
        "          else:\n",
        "            for token_ans in filtered_answer:\n",
        "              if token_ans in item or item in token_ans:\n",
        "                continue\n",
        "\n",
        "              all_answer.append(token_ans)\n",
        "\n",
        "            item_answers.append(' '.join(all_answer))\n",
        "\n",
        "\n",
        "      yes_no_final_answers.append(item_answers)\n",
        "\n",
        "  #ONLY FOR ANIMALS(rats)\n",
        "  if is_animals:\n",
        "    yes_no_final_answers[50][1] = 'are vertabrates'\n",
        "\n",
        "  return yes_no_final_answers"
      ],
      "metadata": {
        "id": "uonn06jMtV2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate answer from question-answer transformer\n",
        "def get_qna(questions, answers):\n",
        "  new_answers = []\n",
        "\n",
        "  for i, item in enumerate(items):\n",
        "      item_answers = []\n",
        "\n",
        "      for j in range(num_questions):\n",
        "        try:\n",
        "          item_answers.append(qa_model(question = questions[i][j], context = answers[i][j])['answer'])\n",
        "        except:\n",
        "          print(f\"error occured, i: {i}, j:{j}, question: {questions[i][j]} context: {answers[i][j]}\")\n",
        "\n",
        "\n",
        "      new_answers.append(item_answers)\n",
        "\n",
        "  return new_answers\n"
      ],
      "metadata": {
        "id": "fn6X56E7RoAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get elements above threshold qa\n",
        "def get_best_keywords(items, questions, answers, new_answers):\n",
        "  best_keywords = []\n",
        "\n",
        "  for i, item in enumerate(items):\n",
        "      keywords_answers = []\n",
        "      for j in range(num_questions):\n",
        "          if len(new_answers[i][j].split()) <= 0.30 * len(answers[i][j].split()):\n",
        "            best_similarity = new_answers[i][j].split()\n",
        "          else:\n",
        "            best_similarity = extract_best_similarity(item, questions[i][j], new_answers[i][j], model, threshold = 0.60)\n",
        "\n",
        "          double_results = []\n",
        "\n",
        "          for elem in best_similarity:\n",
        "            if elem not in double_results:\n",
        "              double_results.append(elem)\n",
        "          keywords_answers.append(double_results)\n",
        "\n",
        "      best_keywords.append(keywords_answers)\n",
        "\n",
        "  return best_keywords"
      ],
      "metadata": {
        "id": "BiqnOv8--L__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compute best keywords**"
      ],
      "metadata": {
        "id": "ksMe7XmpBv6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yes_no_answers = extract_yes_no_answers(questions, answers, is_animals)\n",
        "qna_answers = get_qna(questions, yes_no_answers)\n",
        "best_keywords = get_best_keywords(items, questions, answers, qna_answers)\n"
      ],
      "metadata": {
        "id": "LWJSEmLRBEzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print result\n",
        "f = open('class_06.values.tsv', 'w+')\n",
        "\n",
        "for i, item in enumerate(items):\n",
        "    item_answers = []\n",
        "    for j in range(num_questions):\n",
        "      keyword = ''\n",
        "      for idx, elem in enumerate(best_keywords[i][j]):\n",
        "        if idx > 0:\n",
        "          keyword += ';'\n",
        "        keyword += elem\n",
        "      f.write(f'{item} \\t {questions[i][j]} \\t {keyword} \\n')\n"
      ],
      "metadata": {
        "id": "YAbG-XQ6Ledz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer**"
      ],
      "metadata": {
        "id": "re7sDbyQDpY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert with all questions of a specific item"
      ],
      "metadata": {
        "id": "6QXGnSWMDsoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model_bert = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
      ],
      "metadata": {
        "id": "hZXpIC9jDroG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(w1, w2):\n",
        "\n",
        "  norm1 = np.linalg.norm(w1)\n",
        "  norm2 = np.linalg.norm(w2)\n",
        "\n",
        "  if norm1 == 0 or norm2 == 0:\n",
        "    print(\"norms equal 0\")\n",
        "    return 0\n",
        "\n",
        "  cos_sim = np.dot(w1, w2.T) / (norm1 * norm2)\n",
        "  return cos_sim"
      ],
      "metadata": {
        "id": "q5sZV2OvNauL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mappings(split_question, split_answer):\n",
        "  #mapp idx token to original question\n",
        "  map_idx_question = []\n",
        "  for i, token in enumerate(split_question):\n",
        "    tokenized_word = tokenizer_bert.tokenize(token)\n",
        "    # print(tokenized_word)\n",
        "    map_idx_question += (np.ones(len(tokenized_word), dtype=int) * i).tolist()\n",
        "\n",
        "  #mapp idx token to original question\n",
        "  map_idx_answer = []\n",
        "  for i, token in enumerate(split_answer):\n",
        "    tokenized_word = tokenizer_bert.tokenize(token)\n",
        "    # print(tokenized_word)\n",
        "    map_idx_answer += (np.ones(len(tokenized_word), dtype=int) * i).tolist()\n",
        "\n",
        "  return map_idx_question, map_idx_answer"
      ],
      "metadata": {
        "id": "ztGosHHSMQTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bert_embeddings(question, answer):\n",
        "\n",
        "  tokenized_question = tokenizer_bert(question, return_tensors=\"pt\")\n",
        "  tokenized_answer = tokenizer_bert(answer, return_tensors=\"pt\")\n",
        "\n",
        "  output_question = model_bert(**tokenized_question)\n",
        "  output_answer = model_bert(**tokenized_answer)\n",
        "\n",
        "  outputs_question_sum = torch.stack(output_question.hidden_states[-4:], dim=0).sum(dim=0)\n",
        "  outputs_answer_sum = torch.stack(output_answer.hidden_states[-4:], dim=0).sum(dim=0)\n",
        "\n",
        "  return outputs_question_sum, outputs_answer_sum"
      ],
      "metadata": {
        "id": "zj0HwWnqMWs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batches = []\n",
        "mappings = []\n",
        "\n",
        "for i in range(num_questions): #all the questions for a specific item\n",
        "\n",
        "  preprocessed_question = preprocess_text(questions[0][i], stop_words, True)\n",
        "  preprocessed_answer = preprocess_text(answers[0][i], stop_words, True)\n",
        "\n",
        "  map_idx_question, map_idx_answer = compute_mappings(preprocessed_question, preprocessed_answer)\n",
        "  mappings.append({\n",
        "      'map_idx_question': map_idx_question,\n",
        "      'map_idx_answer': map_idx_answer,\n",
        "      'preprocessed_question': preprocessed_question,\n",
        "      'preprocessed_answer': preprocessed_answer\n",
        "  })\n",
        "  batches.append(' '.join(preprocessed_question) + ' ' + ' '.join(preprocessed_answer))\n"
      ],
      "metadata": {
        "id": "wLiqgYddMubA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer_bert(batches, return_tensors=\"pt\", padding=True)\n",
        "output = model_bert(**tokenized)\n",
        "# last_hidden_states = output.last_hidden_state\n",
        "\n",
        "last_hidden_states = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)"
      ],
      "metadata": {
        "id": "OslgPoUSMwwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(last_hidden_states.shape[0]):\n",
        "\n",
        "  batch = last_hidden_states[idx]\n",
        "\n",
        "  map_idx_question = mappings[idx]['map_idx_question']\n",
        "  map_idx_answer = mappings[idx]['map_idx_answer']\n",
        "  preprocessed_question = mappings[idx]['preprocessed_question']\n",
        "  preprocessed_answer = mappings[idx]['preprocessed_answer']\n",
        "\n",
        "  offset_question = 1\n",
        "  offset_answer = len(map_idx_question) + 1\n",
        "\n",
        "  avg_general = []\n",
        "\n",
        "  for i, token_answer in enumerate(preprocessed_answer): #answers\n",
        "    similarities = []\n",
        "\n",
        "    for j, token_question in enumerate(preprocessed_question):\n",
        "\n",
        "      start_idx_answer = map_idx_answer.index(i)\n",
        "      start_idx_question = map_idx_question.index(j)\n",
        "\n",
        "      count_i = map_idx_answer.count(i)\n",
        "      count_j = map_idx_question.count(j)\n",
        "\n",
        "      offset_start_answer = start_idx_answer + 1 + len(map_idx_question)\n",
        "      offset_start_question = start_idx_question + 1\n",
        "\n",
        "      offset_end_answer = offset_start_answer + count_i\n",
        "      offset_end_question = offset_start_question + count_j\n",
        "\n",
        "      phrase_token = tokenizer_bert.tokenize(batches[idx])\n",
        "      phrase_token_cls = ['[CLS]'] + phrase_token\n",
        "\n",
        "      answer_tok = last_hidden_states[idx, offset_start_answer: offset_end_answer]\n",
        "      question_tok = last_hidden_states[idx, offset_start_question: offset_end_question]\n",
        "\n",
        "      ans_tok = answer_tok.mean(dim=0) # word embedding for word i in answer\n",
        "      quest_tok = question_tok.mean(dim=0) # word embedding for word i in answer\n",
        "\n",
        "      similarity = compute_similarity(ans_tok.detach().numpy(), quest_tok.detach().numpy())\n",
        "      similarities.append(similarity)\n",
        "\n",
        "      # encoded = tokenizer_bert(batches[idx])\n",
        "      # decoded = tokenizer_bert.decode(encoded['input_ids'])\n",
        "      # decoded_split = decoded.split()\n",
        "      # phrase_ = tokenizer_bert.tokenize(batches[idx])\n",
        "\n",
        "      # print(phrase_token)\n",
        "      # print(answer_tok.shape)\n",
        "      # print(quest_tok.shape)\n",
        "      # print(ans_tok.shape)\n",
        "      # print(quest_tok.shape)\n",
        "      # print(encoded)\n",
        "      # print(decoded)\n",
        "      # print(decoded_split)\n",
        "\n",
        "\n",
        "      # print(f'answer_token: {phrase_token[offset_start_answer: offset_end_answer]}')\n",
        "      # print(f'question_token: {phrase_token[offset_start_question: offset_end_question]}')\n",
        "\n",
        "      print(phrase_token_cls)\n",
        "\n",
        "      print(f'answer_token decoded: {phrase_token_cls[offset_start_answer: offset_end_answer]}')\n",
        "      print(f'question_token decoded: {phrase_token_cls[offset_start_question: offset_end_question]}')\n",
        "      print(f'similarity: {similarity}')\n",
        "      print()\n",
        "      # print()\n",
        "\n",
        "      # break\n",
        "    avg_general.append(np.array(similarities).mean())\n",
        "    print(\"---------------------------------\")\n",
        "    print(f'similarity mean for {phrase_token_cls[offset_start_answer: offset_end_answer]} is: {np.array(similarities).mean()}')\n",
        "    print(\"---------------------------------\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # break\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "id": "uO2A9-EqM6zX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}