{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNtbaQHcWDz5"
      },
      "source": [
        "# **Natural Language Processing Homework 2: Word Sense Ambiguitous (WSD)**\n",
        "\n",
        "*Authors:*\n",
        "\n",
        "*   Lorenzo Ciarpaglini (student ID: 1813738)\n",
        "\n",
        "This is the notebook running my implementation of the WSD task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5TXd1-PmeIF",
        "tags": []
      },
      "source": [
        "# Preliminary\n",
        "\n",
        "Run this section to import and/or download the libraries needed for the proper functioning of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-7FFAHRd2xz",
        "outputId": "d68e8499-fc6a-408e-da3b-8879dff44478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp2023-hw2'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 29 (delta 3), reused 2 (delta 2), pack-reused 20\u001b[K\n",
            "Receiving objects: 100% (29/29), 11.47 MiB | 15.26 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SapienzaNLP/nlp2023-hw2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMnm8haMze0P",
        "outputId": "fc73f636-4840-4d37-943f-b04b26d3fda3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m440.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=f96ac529efaff7e206822d77163f1fdac5a3935cd1b832fc6b335fdd53f25d48\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting gdown==4.6.0\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.6.0) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install sklearn\n",
        "!pip install seqeval\n",
        "!pip install sentencepiece # used by huggingface\n",
        "!pip install transformers # install huggingface transformers\n",
        "\n",
        "!pip install gdown==4.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y52_ijzc47pG",
        "outputId": "5c0b3234-73ec-4249-a85b-5b926d18964e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aTHMsGote25X48Kk85RGY7LD7UiVPEhk\n",
            "To: /content/vocabulary_fine.json\n",
            "100% 1.05M/1.05M [00:00<00:00, 94.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mb3w8Pk6n74R4avHdgoHIKlRwuIMbYXW\n",
            "To: /content/vocabulary_coarse.json\n",
            "100% 963k/963k [00:00<00:00, 106MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1aTHMsGote25X48Kk85RGY7LD7UiVPEhk\n",
        "!gdown 1mb3w8Pk6n74R4avHdgoHIKlRwuIMbYXW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UuoWrolpjZ6",
        "outputId": "02014de6-cc06-43a7-f846-455cf2ffe078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "7#import libraries here\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import math\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List #, Union, Set, Callable\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, BertForTokenClassification\n",
        "\n",
        "import json\n",
        "\n",
        "from seqeval.metrics import accuracy_score\n",
        "from seqeval.metrics import classification_report\n",
        "from seqeval.metrics import f1_score\n",
        "from seqeval.metrics import precision_score\n",
        "from seqeval.metrics import recall_score\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files as gg_files\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "SEED:int = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcEQOEhkgM6f"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# model_bert_context = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "# model_bert_gloss = AutoModel.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7EXkJ94Orgf"
      },
      "outputs": [],
      "source": [
        "#some global parameters & constants\n",
        "DATASET_DIR = \"nlp2023-hw2/data\"\n",
        "PRINT_BAR = '-' * 10\n",
        "\n",
        "file_type_dir = \"json\"  #or \"xml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPV91ZpgDf76"
      },
      "outputs": [],
      "source": [
        "def extract_file(path, to_directory='.'):\n",
        "    if path.endswith('.zip'):\n",
        "        opener, mode = zipfile.ZipFile, 'r'\n",
        "    elif path.endswith('.tar.gz') or path.endswith('.tgz'):\n",
        "        opener, mode = tarfile.open, 'r:gz'\n",
        "    elif path.endswith('.tar.bz2') or path.endswith('.tbz'):\n",
        "        opener, mode = tarfile.open, 'r:bz2'\n",
        "    else:\n",
        "        print(\"Error\")\n",
        "        # raise ValueError, \"Could not extract `%s` as no appropriate extractor is found\" % path\n",
        "\n",
        "    cwd = os.getcwd()\n",
        "    os.chdir(to_directory)\n",
        "\n",
        "    try:\n",
        "        file = opener(path, mode)\n",
        "        try: file.extractall()\n",
        "        finally: file.close()\n",
        "    finally:\n",
        "        print(\"Finally\")\n",
        "        os.chdir(cwd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpdqQroSEBtI",
        "outputId": "f618f96c-ae5d-4393-ef74-e8b600ad967b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finally\n"
          ]
        }
      ],
      "source": [
        "extract_file(\"data.tar.gz\", \"nlp2023-hw2/data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4UninTAG-3l",
        "tags": []
      },
      "source": [
        "# DatasetDownloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxlqdCqqG-3l"
      },
      "outputs": [],
      "source": [
        "class DatasetDownloader():\n",
        "    '''\n",
        "      The DatasetDownloader class set all imported vocabulary pre-created, or creates it from scratch, to map lemmas, pos and tags to their relative indices.\n",
        "      Sets also the glossary retrieved from the coarse_fine_defs_map.json file needed to map clusters to fine grained.\n",
        "      Creates a mapping between the original sentence words to the tokens obtained by a tokenizer for a transformer.\n",
        "      Creates the candidates mask needed to filter out only the target word to be disambiguated and their possible candidates.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.sentences = []\n",
        "        self.data_set = []\n",
        "        self.glossary = {}\n",
        "        self.map_grained_to_fine = {}\n",
        "        self.map_fine_to_grained = {}\n",
        "\n",
        "        self.lemmas2idx = {}\n",
        "        self.tags2idx = {}\n",
        "        self.pos2idx = {}\n",
        "\n",
        "        self.idx2lemmas = {}\n",
        "        self.idx2tags = {}\n",
        "        self.idx2pos = {}\n",
        "\n",
        "        self.bep_dicts = []\n",
        "\n",
        "    def getDictionary(self) -> Tuple[Dict, Dict, Dict, Dict, Dict, Dict]:\n",
        "        '''\n",
        "        Return all the dictionaries needed to map all the lemmas, pos and tags to indices and vice-versa\n",
        "        '''\n",
        "        return self.lemmas2idx,self.tags2idx,self.pos2idx,self.idx2lemmas,self.idx2tags, self.idx2pos\n",
        "\n",
        "    def setDictionary(self,\n",
        "                      lemmas2idx: Dict,\n",
        "                      tags2idx: Dict,\n",
        "                      pos2idx: Dict,\n",
        "                      idx2lemmas: Dict,\n",
        "                      idx2tags: Dict,\n",
        "                      idx2pos: Dict) -> None:\n",
        "        '''\n",
        "        Set all the dictionaries needed to map all the lemmas, pos and tags to indices and vice-versa\n",
        "        '''\n",
        "\n",
        "        self.lemmas2idx = lemmas2idx\n",
        "        self.tags2idx = tags2idx\n",
        "        self.pos2idx = pos2idx\n",
        "        self.idx2lemmas = idx2lemmas\n",
        "        self.idx2tags = idx2tags\n",
        "        self.idx2pos = idx2pos\n",
        "\n",
        "        return\n",
        "\n",
        "    def setGlossary(self, glossary: Dict) -> None:\n",
        "        '''\n",
        "          Set the glossary, which is the mapping between the clusters and the Ground truth\n",
        "        '''\n",
        "\n",
        "        self.glossary = glossary\n",
        "\n",
        "        return\n",
        "\n",
        "    def getBertDictionary(self) -> None:\n",
        "        '''\n",
        "        Return an array mapping for each sentences all words from the original words to the tokens generated by a transformer tokenizer\n",
        "        '''\n",
        "        return self.bep_dicts\n",
        "\n",
        "    def set_data(self,\n",
        "                 DATASET_DIR: str,\n",
        "                 DATASET_PREFIX: str,\n",
        "                 DATASET_FILE: str,\n",
        "                 FILE_TYPE: str,\n",
        "                 MAX_LENGTH: int)  -> None:\n",
        "\n",
        "        '''\n",
        "        Download the dataset and remove all the sentences with a length greater than MAX_LENGTH, because sentences too big increase too much the batch size.\n",
        "        '''\n",
        "\n",
        "        self.downloadDataset(DATASET_DIR, DATASET_PREFIX, DATASET_FILE, FILE_TYPE)\n",
        "        self.removeWordLongerThan(MAX_LENGTH)\n",
        "        return\n",
        "\n",
        "    def set_data_from_sentences(self,\n",
        "                 sentences: str,\n",
        "                 MAX_LENGTH: int)  -> None:\n",
        "\n",
        "        '''\n",
        "        Download the dataset and remove all the sentences with a length greater than MAX_LENGTH.\n",
        "        '''\n",
        "\n",
        "        self.getDatasetFromSentences(sentences)\n",
        "        self.removeWordLongerThan(MAX_LENGTH)\n",
        "        return\n",
        "\n",
        "    def downloadMapGrainedFine(self, DATASET_DIR: str) -> None:\n",
        "        '''\n",
        "          Download all the coarse to fine mapping, and create also an inverse map Dict\n",
        "        '''\n",
        "        data_path = os.path.join(DATASET_DIR, 'map', 'coarse_fine_defs_map.json')\n",
        "        with open(data_path) as f:\n",
        "            dictionary = f.read()\n",
        "            dictionary = json.loads(dictionary)\n",
        "\n",
        "            self.map_grained_to_fine = dictionary\n",
        "\n",
        "        for sense_cluster, senses in self.map_grained_to_fine.items():\n",
        "            for idx_sense, sense in enumerate(senses):\n",
        "                for key, value in sense.items():\n",
        "                    if key not in self.map_fine_to_grained:\n",
        "                        self.map_fine_to_grained[key] = {\n",
        "                            'cluster': sense_cluster,\n",
        "                            'definition':  value\n",
        "                        }\n",
        "        return\n",
        "\n",
        "    def downloadDataset(self, DATASET_DIR: str, DATASET_PREFIX: str, DATASET_FILE: str, FILE_TYPE: str) -> None:\n",
        "        '''\n",
        "        Create and array with all elements are dictionaries taken from the file.\n",
        "        We also add the id, e.g 'id' = \"d011.s152\"\n",
        "        '''\n",
        "        data_path = os.path.join(DATASET_DIR, DATASET_PREFIX)\n",
        "        data_path = os.path.join(data_path, DATASET_FILE)\n",
        "        data_path += '.' + FILE_TYPE\n",
        "\n",
        "        with open(data_path) as f:\n",
        "            dictionary = f.read()\n",
        "            dictionary = json.loads(dictionary)\n",
        "\n",
        "            for idx, value in dictionary.items():\n",
        "              value['id'] = idx\n",
        "              self.sentences.append(value)\n",
        "\n",
        "        return\n",
        "\n",
        "    def getDatasetFromSentences(self, sentences: List[str]) -> None:\n",
        "        '''\n",
        "        Create and array with all elements are dictionaries taken from the file.\n",
        "        We also add the id, e.g 'id' = \"d011.s152\"\n",
        "        '''\n",
        "\n",
        "        self.sentences = sentences\n",
        "        return\n",
        "\n",
        "    def getLongestAndAvgSentence(self) -> Tuple[int, int, float]:\n",
        "        '''\n",
        "        Compute the longest and average length of all sentences\n",
        "        '''\n",
        "        longest, idx_longest, avg = 0, 0, 0\n",
        "\n",
        "        for i, data in enumerate(self.sentences):\n",
        "            if longest < len(data['words']):\n",
        "                longest = len(data['words'])\n",
        "                idx_longest = i\n",
        "            avg += len(data['words'])\n",
        "\n",
        "        return longest, idx_longest, avg/(i+1)\n",
        "\n",
        "    def countWordsLongerAvg(self, avg: int) -> int:\n",
        "        '''\n",
        "        Count the number of sentences greater than the average length of all sentences\n",
        "        '''\n",
        "        count = 0\n",
        "        for i, data in enumerate(self.sentences):\n",
        "            if avg < len(data['words']):\n",
        "                count += 1\n",
        "        return count\n",
        "\n",
        "    def removeWordLongerThan(self, max_length: int) -> None:\n",
        "        '''\n",
        "        Remove all the sentences longer than max_length\n",
        "        '''\n",
        "        sentences_no_longer = []\n",
        "        for i, data in enumerate(self.sentences):\n",
        "            if max_length > len(data['words']):\n",
        "                sentences_no_longer.append(data)\n",
        "\n",
        "        self.sentences = sentences_no_longer\n",
        "        return\n",
        "\n",
        "    def retrieveGlossaryFromCandidates(self) -> None:\n",
        "        '''\n",
        "        Get all the senses definitions for all the senses of the correct cluster\n",
        "        for each word to disambiguate\n",
        "        '''\n",
        "\n",
        "        for index, data in tqdm(enumerate(self.sentences)):\n",
        "            # data = json.loads(data)\n",
        "            glosses = {}\n",
        "\n",
        "            for key, candidates in data['candidates'].items():\n",
        "                glosses[key] = []\n",
        "\n",
        "                for candidate in candidates:\n",
        "\n",
        "                    if candidate not in self.glossary.map_fine_to_grained:\n",
        "                        print(f'candidate {candidate} not present in the mapping')\n",
        "                        continue\n",
        "\n",
        "                    #Retrieve the relative sense definition\n",
        "                    sense = self.glossary.map_fine_to_grained[candidate]\n",
        "                    glosses[key].append(sense['definition'])\n",
        "\n",
        "                self.sentences[index]['glosses'] = glosses\n",
        "        return\n",
        "\n",
        "    def retrieveGlossaryFromSenses(self) -> None:\n",
        "        '''\n",
        "        Get all the senses definitions for all the senses of the correct cluster\n",
        "        for each word to disambiguate\n",
        "        '''\n",
        "\n",
        "        for index, data in tqdm(enumerate(self.sentences)):\n",
        "            # data = json.loads(data)\n",
        "            glosses = {}\n",
        "\n",
        "            for key, senses in data['senses'].items():\n",
        "                glosses[key] = []\n",
        "\n",
        "                for sense in senses: #it's just 1\n",
        "\n",
        "                    if sense not in self.glossary.map_grained_to_fine:\n",
        "                        print(f'Sense {sense} not present in the mapping')\n",
        "                        continue\n",
        "\n",
        "                    #Alternative with alldictionary elements\n",
        "                    # #Retrieve the relative sense definition\n",
        "                    # for idx_sense, sense_def in enumerate(self.map_grained_to_fine[sense]):\n",
        "                    #     glosses[key].append(sense_def)\n",
        "\n",
        "                    #Retrieve the relative sense definition\n",
        "                    for idx_sense_arr, sense_def_dict in enumerate(self.glossary.map_grained_to_fine[sense]):\n",
        "                        for idx_sense, sense_def in sense_def_dict.items():\n",
        "                            glosses[key].append(sense_def)\n",
        "\n",
        "                self.sentences[index]['glosses'] = glosses\n",
        "        return\n",
        "\n",
        "    def createVocabolary(self) -> None:\n",
        "        '''\n",
        "        We create a dictionary(word: int) with all the tokens present all the sentences(a vocabulary)\n",
        "        for both the lemmas and the tags(candidates, e.g. \"snarl.v.h.01\").\n",
        "        Then we add also 'PAD' and 'UNK' tokens in both the vocabularies.\n",
        "        Finally we create an inverse mapping (int: word), so given the index we can get the word\n",
        "        '''\n",
        "        # words_set = set()\n",
        "        words_set = []\n",
        "        tags_set = []\n",
        "\n",
        "        for index, data in tqdm(enumerate(self.sentences)):\n",
        "            # data = json.loads(data)\n",
        "\n",
        "            for token in data['lemmas']:\n",
        "              # words_set.add(token)\n",
        "              if token not in words_set:\n",
        "                words_set.append(token)\n",
        "\n",
        "            for key, candidates in data['candidates'].items():\n",
        "              # words_set.add(token)\n",
        "              for candidate in candidates:\n",
        "\n",
        "                if candidate not in tags_set:\n",
        "                  tags_set.append(candidate)\n",
        "\n",
        "        self.lemmas2idx['PAD'] = 0\n",
        "        self.lemmas2idx['UNK'] = 1\n",
        "\n",
        "        for idx, word in enumerate(words_set):\n",
        "            self.lemmas2idx[word] = len(self.lemmas2idx)\n",
        "\n",
        "        # vocab['UNK'] = idx + 1\n",
        "        # vocab['PAD'] = idx + 2\n",
        "\n",
        "        self.tags2idx['PAD'] = 0\n",
        "        self.tags2idx['UNK'] = 1\n",
        "\n",
        "        for idx, tag in enumerate(tags_set):\n",
        "            self.tags2idx[tag] = len(self.tags2idx)\n",
        "\n",
        "        for key, value in self.lemmas2idx.items():\n",
        "            self.idx2lemmas[value] = key\n",
        "\n",
        "        for key, value in self.tags2idx.items():\n",
        "            self.idx2tags[value] = key\n",
        "\n",
        "        return\n",
        "\n",
        "    def createVocabolaryPos(self) -> None:\n",
        "        '''\n",
        "        We create a dictionary(tag: int) with all the POS tags(a vocabulary)\n",
        "        Then we add also 'PAD' token.\n",
        "        We create also an inverse mapping (int: tag).\n",
        "        '''\n",
        "        # words_set = set()\n",
        "        pos_set = []\n",
        "\n",
        "        for index, data in tqdm(enumerate(self.sentences)):\n",
        "        # data = json.loads(data)\n",
        "\n",
        "            for pos in data['pos_tags']:\n",
        "              if pos not in pos_set:\n",
        "                pos_set.append(pos)\n",
        "\n",
        "        self.pos2idx['PAD'] = 0\n",
        "\n",
        "        for idx, pos in enumerate(pos_set):\n",
        "            self.pos2idx[pos] = len(self.pos2idx)\n",
        "\n",
        "        for key, value in self.pos2idx.items():\n",
        "            self.idx2pos[value] = key\n",
        "\n",
        "        #For unknown pos assign noun, that is the most common, see link utili\n",
        "\n",
        "        return\n",
        "\n",
        "    #Alternative using text_tokenized.word_ids()\n",
        "    def map_origin_word_to_bert(self, words, tokenizer):\n",
        "        '''\n",
        "        Create a mapping between all original words to the tokens generated by a transformer tokenizer\n",
        "        '''\n",
        "        bep_dict = {}\n",
        "        current_idx = 0\n",
        "        for word_idx, word in enumerate(words):\n",
        "            bert_word = tokenizer.tokenize(word)\n",
        "            word_len = len(bert_word)\n",
        "            bep_dict[word_idx] = [current_idx, current_idx + word_len - 1]\n",
        "            current_idx = current_idx + word_len\n",
        "        return bep_dict\n",
        "\n",
        "    def map_to_dict(self, N = None, predict = False) -> None:\n",
        "\n",
        "      '''\n",
        "      This method create for each sentence a mapping between the tokenized sentence and the original sentence.\n",
        "      Since the tokenizer can split a word in multiple tokens, like clockwork in 'clock', '##work',\n",
        "      I need a mapping between the position of a original word in the sub-words.\n",
        "\n",
        "      Then for each token in lemmas, pos_tags, candidates and senses the relative vocabulary index are added\n",
        "      if they are present else 'unk' index is added e.g. '1'\n",
        "      '''\n",
        "\n",
        "\n",
        "      self.data_set = [{} for i in range(len(self.sentences))]\n",
        "      non_in_tags = 0\n",
        "\n",
        "      for i, data in tqdm(enumerate(self.sentences[:N])):\n",
        "\n",
        "        self.data_set[i]['words'] = data['words']\n",
        "        self.data_set[i]['lemmas'] = []\n",
        "        self.data_set[i]['pos'] = []\n",
        "        self.data_set[i]['candidates'] = {}\n",
        "        self.data_set[i]['original_candidates'] = self.sentences[i]['candidates']\n",
        "        self.data_set[i]['glosses'] = self.sentences[i]['glosses']\n",
        "\n",
        "        if predict is False:\n",
        "            self.data_set[i]['senses'] = {}\n",
        "            self.data_set[i]['original_senses'] = self.sentences[i]['senses']\n",
        "\n",
        "        words_joined = ' '.join(data['words'])\n",
        "        words_tokenized = tokenizer.tokenize(words_joined)\n",
        "\n",
        "        bep_dict = self.map_origin_word_to_bert(data['words'], tokenizer)\n",
        "        self.bep_dicts.append(bep_dict)\n",
        "\n",
        "        for token in data['lemmas']:\n",
        "          self.data_set[i]['lemmas'].append(self.lemmas2idx[token] if token in self.lemmas2idx else self.lemmas2idx['UNK'])\n",
        "\n",
        "        for pos in data['pos_tags']:\n",
        "          self.data_set[i]['pos'].append(self.pos2idx[pos])  #possible that i encounter a pos tag not in vocab\n",
        "\n",
        "        for key, candidates in data['candidates'].items():\n",
        "          for candidate in candidates:\n",
        "            if key not in self.data_set[i]['candidates']:\n",
        "              self.data_set[i]['candidates'][key] = []\n",
        "            self.data_set[i]['candidates'][key].append(self.tags2idx[candidate] if candidate in self.tags2idx else self.tags2idx['UNK'])\n",
        "            if candidate not in self.tags2idx:\n",
        "              non_in_tags += 1\n",
        "\n",
        "        if predict is False:\n",
        "            for key, senses in data['senses'].items():\n",
        "                for sense in senses:\n",
        "                    if key not in self.data_set[i]['senses']:\n",
        "                        self.data_set[i]['senses'][key] = []\n",
        "                        self.data_set[i]['senses'][key].append(self.tags2idx[sense] if sense in self.tags2idx else self.tags2idx['UNK'])\n",
        "\n",
        "\n",
        "        self.data_set[i]['len'] = len(words_tokenized)\n",
        "      print(f'non in tags: {non_in_tags}')\n",
        "\n",
        "      return\n",
        "\n",
        "    def get_max_len(self):\n",
        "      '''\n",
        "        Return le length of the longest sentence in the dataset.\n",
        "      '''\n",
        "      longest = 0\n",
        "      for i, elem in enumerate(self.data_set):\n",
        "        if elem['len'] > longest:\n",
        "          longest = elem['len']\n",
        "      return longest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pKIIBpilSNN"
      },
      "outputs": [],
      "source": [
        "class GlossaryDownloader():\n",
        "    '''\n",
        "      GlossaryDownloader is responsible to download the mapping between clusters and fine_grained.\n",
        "      It also create and inverse mapping from the fine-grained to the clusters.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.map_grained_to_fine = {}\n",
        "        self.map_fine_to_grained = {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def downloadMapGrainedFine(self, DATASET_DIR: str) -> None:\n",
        "        '''\n",
        "          Download all the coarse to fine mapping, and create also an inverse map Dict\n",
        "        '''\n",
        "        data_path = os.path.join(DATASET_DIR, 'map', 'coarse_fine_defs_map.json')\n",
        "        with open(data_path) as f:\n",
        "            dictionary = f.read()\n",
        "            dictionary = json.loads(dictionary)\n",
        "\n",
        "            self.map_grained_to_fine = dictionary\n",
        "\n",
        "        for sense_cluster, senses in self.map_grained_to_fine.items():\n",
        "            for idx_sense, sense in enumerate(senses):\n",
        "                for key, value in sense.items():\n",
        "                    if key not in self.map_fine_to_grained:\n",
        "                        self.map_fine_to_grained[key] = {\n",
        "                            'cluster': sense_cluster,\n",
        "                            'definition':  value\n",
        "                        }\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giZu-DW5ESy4"
      },
      "outputs": [],
      "source": [
        "def get_max_len(data_set):\n",
        "      '''\n",
        "        Return le length of the longest sentence in the dataset.\n",
        "      '''\n",
        "      longest = 0\n",
        "      for i, elem in enumerate(data_set):\n",
        "        if elem['len'] > longest:\n",
        "          longest = elem['len']\n",
        "      return longest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFAl2R27G-3l",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsRn0j8jG-3l"
      },
      "outputs": [],
      "source": [
        "class DatasetWSD(Dataset):\n",
        "  def __init__(self,\n",
        "               dataset) -> None:\n",
        "\n",
        "    self.data_set = dataset.data_set\n",
        "    self.bep_dicts = dataset.bep_dicts\n",
        "\n",
        "  ''' returns how many entries we have for a specific category '''\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.data_set)\n",
        "\n",
        "  ''' returns one item of one category '''\n",
        "  def __getitem__(self, idx:int) -> dict:\n",
        "    return self.data_set[idx], self.bep_dicts[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUZa2jl0GEh_"
      },
      "outputs": [],
      "source": [
        "#fine_grained\n",
        "def fine_collate_fn(d): #batch, bep_dicts\n",
        "    '''\n",
        "    Each sample of the batch contains the original sentence, original candidates, the labels and all the glosses.\n",
        "    With original is meant before to be passed to a transformer tokenizer.\n",
        "    The glosses contain all the sense definitions for each candidate sense.\n",
        "    '''\n",
        "      data_set = [elem[0] for elem in d]\n",
        "      bep_dict = [elem[1] for elem in d]\n",
        "\n",
        "      batch = []\n",
        "\n",
        "      batches_tokens = []\n",
        "      candidates_list = []\n",
        "      glosses = []\n",
        "      labels = []\n",
        "\n",
        "      max_len = get_max_len(data_set)\n",
        "\n",
        "      for i, data in enumerate(d):\n",
        "        batch.append(copy.deepcopy(data_set[i]))\n",
        "\n",
        "        batches_tokens.append(batch[i]['words'])\n",
        "        candidates_list.append(batch[i]['original_candidates'])\n",
        "        labels.append(batch[i]['original_senses'])\n",
        "        glosses.append(batch[i]['glosses'])\n",
        "\n",
        "      return batches_tokens, candidates_list, glosses, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3-T9gLJG-3l"
      },
      "outputs": [],
      "source": [
        "#coarse_grained\n",
        "def collate_fn(d): #batch, bep_dicts\n",
        "\n",
        "      '''\n",
        "      Each sample of the batch contains:\n",
        "\n",
        "      batches_tokens: the original sentence, batches_lemmas, , batches_pos, target_mask, candidate_mask, candidates_list, glosses\n",
        "      batches_lemmas:  the lemmas mapped into their own dictionary,\n",
        "      batches_pos: the pos tags mapped into their own dictionary,\n",
        "      batches_labels: labels,\n",
        "      target_mask: filter out only the correct sense, for each word to disambiguate(deprecated),\n",
        "      candidate_mask: filter out only the candidate senses, for each word to disambiguate,\n",
        "      candidates_list: The list of all candidate for each word to be disambiguated,\n",
        "      glosses: all the sense definitions for each sense inside each candidate cluster.\n",
        "\n",
        "      '''\n",
        "      data_set = [elem[0] for elem in d]\n",
        "      bep_dict = [elem[1] for elem in d]\n",
        "\n",
        "      batch = []\n",
        "\n",
        "      batches_tokens = []\n",
        "      batches_lemmas = []\n",
        "      batches_labels = []\n",
        "      batches_pos = []\n",
        "      target_mask = []\n",
        "      candidates_list = []\n",
        "      glosses = []\n",
        "\n",
        "      max_len = get_max_len(data_set)\n",
        "\n",
        "      for i, data in enumerate(d):\n",
        "        batch.append(copy.deepcopy(data_set[i]))\n",
        "\n",
        "        batches_tokens.append(batch[i]['words'])\n",
        "        candidates_list.append(batch[i]['candidates'])\n",
        "        glosses.append(batch[i]['glosses'])\n",
        "\n",
        "        batch[i]['lemmas_mapped_bert'] = [lemmas2idx['PAD'] for i in range(max_len + 2)]\n",
        "        batch[i]['pos_mapped_bert'] = [pos2idx['PAD'] for i in range(max_len + 2)]\n",
        "        batch[i]['labels_mapped_bert'] = [tags2idx['PAD'] for i in range(max_len + 2)]\n",
        "        batch[i]['target_mask'] = [0 for i in range(max_len + 2)]\n",
        "\n",
        "        for idx, senses in batch[i]['senses'].items():\n",
        "\n",
        "          idx_bep = bep_dict[i][int(idx)][0]\n",
        "          batch[i]['labels_mapped_bert'][idx_bep + 1] = senses[0] #idx_bep + 1 because first and last elements must be PAD since are relative to CLS and SEP\n",
        "          batch[i]['target_mask'][idx_bep + 1] = 1 #idx_bep + 1 because first and last elements must be PAD since are relative to CLS and SEP\n",
        "\n",
        "\n",
        "\n",
        "        for idx, word in enumerate(batch[i]['words']):\n",
        "\n",
        "          idx_bep_start = bep_dict[i][int(idx)][0]\n",
        "          idx_bep_end = bep_dict[i][int(idx)][1]\n",
        "\n",
        "\n",
        "          for j in range(idx_bep_start, idx_bep_end + 1):\n",
        "\n",
        "            batch[i]['pos_mapped_bert'][j + 1] = batch[i]['pos'][idx] #j + 1 because first and last elements must be PAD since are relative to CLS and SEP\n",
        "            batch[i]['lemmas_mapped_bert'][j + 1] = batch[i]['lemmas'][idx] #j + 1 because first and last elements must be PAD since are relative to CLS and SEP\n",
        "\n",
        "\n",
        "\n",
        "        batches_lemmas.append(torch.LongTensor(batch[i]['lemmas_mapped_bert']))\n",
        "        batches_labels.append(torch.LongTensor(batch[i]['labels_mapped_bert']))\n",
        "        batches_pos.append(torch.LongTensor(batch[i]['pos_mapped_bert']))\n",
        "\n",
        "        #create the candidate mask\n",
        "        target_mask.append(torch.LongTensor(batch[i]['target_mask']).unsqueeze(1).repeat(1, len(tags2idx)))\n",
        "\n",
        "      batches_lemmas = torch.stack(batches_lemmas)\n",
        "      batches_labels = torch.stack(batches_labels)\n",
        "      batches_pos = torch.stack(batches_pos)\n",
        "      target_mask = torch.stack(target_mask)\n",
        "\n",
        "      sizes = list(batches_labels.shape) + [len(tags2idx)]\n",
        "      candidate_mask = torch.zeros(sizes)\n",
        "\n",
        "      for i in range(candidate_mask.shape[0]):\n",
        "        for idx, candidates in batch[i]['candidates'].items():\n",
        "            for index, candidate in enumerate(candidates):\n",
        "              print(candidate)\n",
        "              idx_bep = bep_dict[i][int(idx)][0] + 1 # only the starting token, idx_bep + 1 because first and last elements must be PAD since are relative to CLS and SEP\n",
        "\n",
        "              candidate_mask[i, idx_bep, candidate] = 1\n",
        "\n",
        "\n",
        "\n",
        "      return batches_tokens, batches_lemmas, batches_labels, batches_pos, target_mask, candidate_mask, candidates_list, glosses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_BRG8T5G-3m",
        "tags": []
      },
      "source": [
        "# Instation Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSF_qCfmLetL"
      },
      "source": [
        "## Instantion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdIFx4_Wqmfe"
      },
      "outputs": [],
      "source": [
        "#Download the mapping between coarse and fine grained and create the inverse mapping\n",
        "glossary = GlossaryDownloader()\n",
        "glossary.downloadMapGrainedFine(DATASET_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eNcIruovYwe"
      },
      "outputs": [],
      "source": [
        "# vocabulary  ={\n",
        "#     'lemmas2idx': lemmas2idx,\n",
        "#     'tags2idx': tags2idx,\n",
        "#     'pos2idx': pos2idx,\n",
        "#     'idx2lemmas': idx2lemmas,\n",
        "#     'idx2tags': idx2tags,\n",
        "#     'idx2pos': idx2pos\n",
        "# }\n",
        "\n",
        "# path_file_json_coarse = \"vocabulary_coarse.json\"\n",
        "\n",
        "\n",
        "# with open(path_file_json_coarse, 'w') as file_json:\n",
        "#     json.dump(vocabulary, file_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uJDDJIW74iK"
      },
      "outputs": [],
      "source": [
        "path_file_json_fine = \"vocabulary_fine.json\"\n",
        "path_file_json_coarse = \"vocabulary_coarse.json\"\n",
        "\n",
        "with open(path_file_json_fine, 'r') as file_json:\n",
        "    vocabulary_fine = json.load(file_json)\n",
        "\n",
        "with open(path_file_json_coarse, 'r') as file_json:\n",
        "    vocabulary_coarse = json.load(file_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdNolJ7Iw2wD"
      },
      "outputs": [],
      "source": [
        "#FINE-GRAINED DATASET\n",
        "t_set = DatasetDownloader()\n",
        "te_set = DatasetDownloader()\n",
        "v_set = DatasetDownloader()\n",
        "\n",
        "#Download all the datasets and remove all the sentences with a length grater than 60\n",
        "t_set.set_data(DATASET_DIR, \"fine-grained\", \"train_fine_grained\", file_type_dir, 60) #nlp_2023/data/fine-grained/train_fine_grained.json\n",
        "te_set.set_data(DATASET_DIR, \"fine-grained\", \"test_fine_grained\", file_type_dir, 60)\n",
        "v_set.set_data(DATASET_DIR, \"fine-grained\", \"val_fine_grained\", file_type_dir, 60)\n",
        "\n",
        "#set the glossary\n",
        "t_set.setGlossary(glossary)\n",
        "te_set.setGlossary(glossary)\n",
        "v_set.setGlossary(glossary)\n",
        "\n",
        "#get the glossary for all the datasets\n",
        "t_set.downloadMapGrainedFine(DATASET_DIR)\n",
        "t_set.retrieveGlossaryFromCandidates()\n",
        "\n",
        "te_set.downloadMapGrainedFine(DATASET_DIR)\n",
        "te_set.retrieveGlossaryFromCandidates()\n",
        "\n",
        "v_set.downloadMapGrainedFine(DATASET_DIR)\n",
        "v_set.retrieveGlossaryFromCandidates()\n",
        "\n",
        "#Create a vocabolary for all the words in lemmas, candidate clusters and pos\n",
        "# t_set.createVocabolary()\n",
        "# t_set.createVocabolaryPos()\n",
        "\n",
        "lemmas2idx_fine, tags2idx_fine, pos2idx_fine, idx2lemmas_fine, idx2tags_fine, idx2pos_fine = vocabulary_fine.values()\n",
        "\n",
        "t_set.setDictionary(lemmas2idx_fine, tags2idx_fine, pos2idx_fine, idx2lemmas_fine, idx2tags_fine, idx2pos_fine)\n",
        "te_set.setDictionary(lemmas2idx_fine, tags2idx_fine, pos2idx_fine, idx2lemmas_fine, idx2tags_fine, idx2pos_fine)\n",
        "v_set.setDictionary(lemmas2idx_fine, tags2idx_fine, pos2idx_fine, idx2lemmas_fine, idx2tags_fine, idx2pos_fine)\n",
        "\n",
        "#Generate the mapping between original words and the tokenized ones\n",
        "t_set.map_to_dict()\n",
        "te_set.map_to_dict()\n",
        "v_set.map_to_dict()\n",
        "\n",
        "train_set = DatasetWSD(t_set)\n",
        "test_set = DatasetWSD(te_set)\n",
        "val_set = DatasetWSD(v_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3e0tJXyQm4O"
      },
      "outputs": [],
      "source": [
        "glossary.map_grained_to_fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMvGmeFlG-3m"
      },
      "outputs": [],
      "source": [
        "#COARSED-GRAINED DATASET\n",
        "\n",
        "t_set_coarse = DatasetDownloader()\n",
        "te_set_coarse = DatasetDownloader()\n",
        "v_set_coarse = DatasetDownloader()\n",
        "\n",
        "#Download all the datasets and remove all the sentences with a length grater than 60\n",
        "t_set_coarse.set_data(DATASET_DIR, \"coarse-grained\", \"train_coarse_grained\", file_type_dir, 60) #nlp_2023/data/coarse-grained/train_coarse_grained.json\n",
        "te_set_coarse.set_data(DATASET_DIR, \"coarse-grained\", \"test_coarse_grained\", file_type_dir, 60)\n",
        "v_set_coarse.set_data(DATASET_DIR, \"coarse-grained\", \"val_coarse_grained\", file_type_dir, 60)\n",
        "\n",
        "#set the glossary\n",
        "t_set_coarse.setGlossary(glossary)\n",
        "te_set_coarse.setGlossary(glossary)\n",
        "v_set_coarse.setGlossary(glossary)\n",
        "\n",
        "#get the glossary for all the datasets\n",
        "# t_set_coarse.downloadMapGrainedFine(DATASET_DIR)\n",
        "t_set_coarse.retrieveGlossaryFromSenses()\n",
        "\n",
        "# te_set_coarse.downloadMapGrainedFine(DATASET_DIR)\n",
        "te_set_coarse.retrieveGlossaryFromSenses()\n",
        "\n",
        "# v_set_coarse.downloadMapGrainedFine(DATASET_DIR)\n",
        "v_set_coarse.retrieveGlossaryFromSenses()\n",
        "\n",
        "#Create a vocabolary for all the words in lemmas, candidate clusters and pos\n",
        "\n",
        "# t_set_coarse.createVocabolary()\n",
        "# t_set_coarse.createVocabolaryPos()\n",
        "# lemmas2idx, tags2idx, pos2idx, idx2lemmas, idx2tags, idx2pos = t_set_coarse.getDictionary()\n",
        "\n",
        "lemmas2idx, tags2idx, pos2idx, idx2lemmas, idx2tags, idx2pos = vocabulary_coarse.values()\n",
        "\n",
        "# lemmas2idx = vocabulary_coarse['lemmas2idx']\n",
        "# tags2idx = vocabulary_coarse['tags2idx']\n",
        "# pos2idx = vocabulary_coarse['pos2idx']\n",
        "# idx2lemmas = vocabulary_coarse['idx2lemmas']\n",
        "# idx2tags = vocabulary_coarse['idx2tags']\n",
        "# idx2pos = vocabulary_coarse['idx2pos']\n",
        "\n",
        "\n",
        "t_set_coarse.setDictionary(lemmas2idx, tags2idx, pos2idx, idx2lemmas, idx2tags, idx2pos)\n",
        "te_set_coarse.setDictionary(lemmas2idx, tags2idx, pos2idx, idx2lemmas, idx2tags, idx2pos)\n",
        "v_set_coarse.setDictionary(lemmas2idx, tags2idx, pos2idx, idx2lemmas, idx2tags, idx2pos)\n",
        "\n",
        "#Generate the mapping between original words and the tokenized ones\n",
        "t_set_coarse.map_to_dict()\n",
        "te_set_coarse.map_to_dict()\n",
        "v_set_coarse.map_to_dict()\n",
        "\n",
        "train_set_coarse = DatasetWSD(t_set_coarse)\n",
        "test_set_coarse = DatasetWSD(te_set_coarse)\n",
        "val_set_coarse = DatasetWSD(v_set_coarse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m63WDxUwGc_x"
      },
      "outputs": [],
      "source": [
        "#FINE GRAINED\n",
        "\n",
        "train_dataloader_fine = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=fine_collate_fn\n",
        " )\n",
        "\n",
        "test_dataloader_fine = DataLoader(\n",
        "    test_set,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=fine_collate_fn\n",
        " )\n",
        "\n",
        "val_dataloader_fine = DataLoader(\n",
        "    val_set,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=fine_collate_fn\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3LL4w99G-3m"
      },
      "outputs": [],
      "source": [
        "#COARSE GRAINED\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_set_coarse,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        " )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_set_coarse,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        " )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_set_coarse,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        " )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CxtI8RpG-3o",
        "tags": []
      },
      "source": [
        "# WSDTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IRyFS7XYHOy"
      },
      "source": [
        "## Fine-grained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zboIW4YYqrl"
      },
      "outputs": [],
      "source": [
        "class WSDModuleFine():\n",
        "    '''\n",
        "      This class will handle the train loop.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                hypers,\n",
        "                model,\n",
        "                optimizer,\n",
        "                glossary,\n",
        "                weights_path: str = None\n",
        "                ) -> None:\n",
        "\n",
        "        self.hypers = hypers\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.glossary = glossary\n",
        "\n",
        "        self.train_losses = []\n",
        "\n",
        "        self.cluster_predictions = []\n",
        "        self.cluster_labels = []\n",
        "\n",
        "        if weights_path is not None:\n",
        "            self.model.load_state_dict(torch.load(weights_path, map_location=torch.device(hypers.device)))\n",
        "\n",
        "        return\n",
        "\n",
        "    def train(self, dataloader, epochs = None):\n",
        "\n",
        "        if epochs is None:\n",
        "          epochs = self.hypers.epochs\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.train_losses_epochs = []\n",
        "\n",
        "        criterion = {}\n",
        "        for key in self.glossary.map_fine_to_grained.keys():\n",
        "\t        criterion[key] = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for i, data in tqdm(enumerate(dataloader), total = len(dataloader)):\n",
        "\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                batches_tokens, candidates_list, glosses, labels = data\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                loss, output = self.model(batches_tokens, glosses, criterion, candidates_list, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                self.train_losses.append(loss)\n",
        "\n",
        "\n",
        "\n",
        "                del batches_tokens\n",
        "                del candidates_list\n",
        "                del glosses\n",
        "                del labels\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "                # accuracies.append(accuracy(output, labels).item())\n",
        "\n",
        "                if i % 100 == 0:\n",
        "                  print(\"iteration: \", i, \" loss: \", loss.item())\n",
        "\n",
        "\n",
        "            print(\"epoch: \", epoch, \" loss: \", self.getLossesMean()['train'])\n",
        "\n",
        "            self.train_losses_epochs.append(torch.tensor(self.train_losses).mean())\n",
        "\n",
        "\n",
        "        print(\"End Training\")\n",
        "        return self.train_losses, self.train_losses_epochs\n",
        "\n",
        "    def getPredictionLabels(self, output, candidate_mask, batches_labels = None):\n",
        "\n",
        "      output = output.detach()\n",
        "      batches_labels = batches_labels.detach()\n",
        "\n",
        "      output = F.softmax(output, dim = -1)\n",
        "      output = output * candidate_mask\n",
        "\n",
        "      cluster_predictions = [[] for i in range(output.shape[0])]\n",
        "\n",
        "      cluster_predictions_indices = output.argmax(-1).nonzero(as_tuple=True)\n",
        "\n",
        "      tags_predicted = output.argmax(-1)[cluster_predictions_indices].tolist()\n",
        "\n",
        "      # print(tup)\n",
        "      for i, elem in enumerate(cluster_predictions_indices[0]):\n",
        "        cluster_predictions[elem.item()].append(idx2tags[tags_predicted[i]])\n",
        "\n",
        "      if batches_labels is not None:\n",
        "\n",
        "        cluster_labels = [[] for i in range(output.shape[0])]\n",
        "        cluster_labels_indices = batches_labels.nonzero(as_tuple=True)\n",
        "        labels = batches_labels[cluster_labels_indices].tolist()\n",
        "\n",
        "\n",
        "        for i, elem in enumerate(cluster_labels_indices[0]):\n",
        "          cluster_labels[elem.item()].append(idx2tags[labels[i]])\n",
        "\n",
        "        return cluster_predictions, cluster_labels\n",
        "\n",
        "      return cluster_predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNcCo3-bYJcq"
      },
      "source": [
        "## Coarse-grained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1xZYecSG-3o"
      },
      "outputs": [],
      "source": [
        "class WSDModuleGrained():\n",
        "    '''\n",
        "      This class will handle the train loop.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                hypers,\n",
        "                model,\n",
        "                optimizer,\n",
        "                idx2tags,\n",
        "                weights_path: str = None\n",
        "                ) -> None:\n",
        "\n",
        "        self.hypers = hypers\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.idx2tags = idx2tags\n",
        "\n",
        "        self.train_losses = []\n",
        "\n",
        "        self.cluster_predictions = []\n",
        "        self.cluster_labels = []\n",
        "\n",
        "        if weights_path is not None:\n",
        "            self.model.load_state_dict(torch.load(weights_path, map_location=torch.device(hypers.device)))\n",
        "\n",
        "        return\n",
        "\n",
        "    def train(self, dataloader, epochs = None):\n",
        "\n",
        "        if epochs is None:\n",
        "          epochs = self.hypers.epochs\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.train_losses_epochs = []\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for i, data in tqdm(enumerate(dataloader), total = len(dataloader)):\n",
        "\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                batches_tokens, batches_lemmas, batches_labels, batches_pos, target_mask, candidate_mask, candidate_list, glosses = data\n",
        "\n",
        "                batches_lemmas = batches_lemmas.to(hypers.device)\n",
        "                batches_labels = batches_labels.to(hypers.device)\n",
        "                batches_pos = batches_pos.to(hypers.device)\n",
        "                target_mask = target_mask.to(hypers.device)\n",
        "                candidate_mask = candidate_mask.to(hypers.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                loss, output = self.model(batches_tokens, batches_lemmas, batches_pos, target_mask, batches_labels, glosses)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                self.train_losses.append(loss)\n",
        "\n",
        "\n",
        "                batches_lemmas.detach()\n",
        "                batches_labels.detach()\n",
        "                batches_pos.detach()\n",
        "                target_mask.detach()\n",
        "                candidate_mask.detach()\n",
        "\n",
        "                del batches_lemmas\n",
        "                del batches_labels\n",
        "                del batches_pos\n",
        "                del target_mask\n",
        "                del candidate_mask\n",
        "                del candidate_list\n",
        "                del glosses\n",
        "\n",
        "                # accuracies.append(accuracy(output, labels).item())\n",
        "\n",
        "                if i % 100 == 0:\n",
        "                  print(\"iteration: \", i, \" loss: \", loss.item())\n",
        "\n",
        "\n",
        "            print(\"epoch: \", epoch, \" loss: \", self.getLossesMean()['train'])\n",
        "            self.train_losses_epochs.append(torch.tensor(self.train_losses).mean())\n",
        "\n",
        "\n",
        "        print(\"End Training\")\n",
        "        return self.train_losses, self.train_losses_epochs\n",
        "\n",
        "    def getPredictionLabels(self, output, candidate_mask, batches_labels = None):\n",
        "\n",
        "      output = output.detach()\n",
        "\n",
        "      output = F.softmax(output, dim = -1)\n",
        "      output = output * candidate_mask\n",
        "\n",
        "      cluster_predictions = [[] for i in range(output.shape[0])]\n",
        "\n",
        "      cluster_predictions_indices = output.argmax(-1).nonzero(as_tuple=True)\n",
        "\n",
        "      tags_predicted = output.argmax(-1)[cluster_predictions_indices].tolist()\n",
        "\n",
        "      # print(tup)\n",
        "      for i, elem in enumerate(cluster_predictions_indices[0]):\n",
        "        cluster_predictions[elem.item()].append(self.idx2tags[str(tags_predicted[i])])\n",
        "\n",
        "      if batches_labels is not None:\n",
        "\n",
        "        batches_labels = batches_labels.detach()\n",
        "\n",
        "\n",
        "        cluster_labels = [[] for i in range(output.shape[0])]\n",
        "        cluster_labels_indices = batches_labels.nonzero(as_tuple=True)\n",
        "        labels = batches_labels[cluster_labels_indices].tolist()\n",
        "\n",
        "\n",
        "        for i, elem in enumerate(cluster_labels_indices[0]):\n",
        "          cluster_labels[elem.item()].append(self.idx2tags[str(labels[i])])\n",
        "\n",
        "        return cluster_predictions, cluster_labels\n",
        "\n",
        "      return cluster_predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0gjCvJ0zHRd",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XMAgxICOdM7"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters for the fine grained model\n",
        "class hypers_fine:\n",
        "    vocab_size = len(lemmas2idx_fine)\n",
        "\n",
        "    vocab_pos_size = len(pos2idx_fine)\n",
        "\n",
        "    embedding_dim = 768 #512\n",
        "\n",
        "    number_of_tags = len(tags2idx_fine)\n",
        "\n",
        "    input_size = 768\n",
        "\n",
        "    learning_rate = 1e-4 #2e-4\n",
        "\n",
        "    batch_size = 8\n",
        "\n",
        "    epochs = 1\n",
        "\n",
        "    dropout_rate = 0.25#0.3\n",
        "\n",
        "    normal_loss_function = False\n",
        "\n",
        "    bertForTokenClassificationLoss = True\n",
        "\n",
        "    print_step = 100\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okUmOzqqNsXi"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters for the coarse grained model\n",
        "class hypers:\n",
        "    vocab_size = len(lemmas2idx)\n",
        "\n",
        "    vocab_pos_size = len(pos2idx)\n",
        "\n",
        "    embedding_dim = 768 #512\n",
        "\n",
        "    number_of_tags = len(tags2idx)\n",
        "\n",
        "    input_size = 768\n",
        "\n",
        "    learning_rate = 1e-4 #2e-4\n",
        "\n",
        "    batch_size = 8\n",
        "\n",
        "    epochs = 1\n",
        "\n",
        "    dropout_rate = 0.25#0.3\n",
        "\n",
        "    normal_loss_function = False\n",
        "\n",
        "    bertForTokenClassificationLoss = True\n",
        "\n",
        "    print_step = 100\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPePshLRuW5H",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exsqg77-xhGJ"
      },
      "source": [
        "## Fine Grained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcRJ07qrxdOC"
      },
      "outputs": [],
      "source": [
        "#Gloss WSD on Fine-grained dataset\n",
        "\n",
        "class GlossWSDFine(nn.Module):\n",
        "\n",
        "    def __init__(self, hypers):\n",
        "        super(GlossWSDFine, self).__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "        self.model_bert_context = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "        self.model_bert_gloss = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "        self.embedding_lemmas = nn.Embedding(hypers.vocab_size + 1, hypers.embedding_dim, padding_idx = lemmas2idx['PAD'])\n",
        "        self.embedding_pos = nn.Embedding(hypers.vocab_pos_size + 1, hypers.embedding_dim, padding_idx = pos2idx['PAD'])\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hypers.embedding_dim)\n",
        "\n",
        "        self.classifier = nn.Linear(hypers.embedding_dim, hypers.number_of_tags)\n",
        "\n",
        "        self.dropout = nn.Dropout(hypers.dropout_rate)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.hypers = hypers\n",
        "\n",
        "    def forward(self, batches_tokens, glosses = None, criterion = None, candidates_list = None, labels = None):\n",
        "\n",
        "        x = tokenizer(batches_tokens, return_tensors=\"pt\",\n",
        "                                  padding='longest',\n",
        "                                  is_split_into_words=True).to(hypers.device)\n",
        "\n",
        "        attention_mask = x['attention_mask'].squeeze(1).to(hypers.device)\n",
        "        input_ids = x['input_ids'].squeeze(1).to(hypers.device)\n",
        "\n",
        "        outputs = self.model_bert_context(input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "\n",
        "        loss = 0\n",
        "        total_length = 0\n",
        "        predictions = []\n",
        "        for b_g_idx, b_g in enumerate(glosses): #b_g = batch_gloss\n",
        "            prediction_batch = []\n",
        "            for index_gloss, gloss in b_g.items():\n",
        "                gloss_arr = [[g] for g in gloss]\n",
        "                gloss_embed = tokenizer(gloss_arr, return_tensors=\"pt\",\n",
        "                                padding='longest',\n",
        "                                is_split_into_words=True).to(hypers.device)\n",
        "\n",
        "                outputs_gloss = self.model_bert_gloss(**gloss_embed).last_hidden_state #senses X max_len X 768\n",
        "\n",
        "\n",
        "                cls_embeddings = outputs_gloss[:, 0, :] #senses X 768\n",
        "\n",
        "                word_ids = tokenizer(batches_tokens[b_g_idx], return_tensors=\"pt\",\n",
        "                                    padding='longest',\n",
        "                                    is_split_into_words=True).word_ids()\n",
        "\n",
        "                word_indexes = np.where(np.array(word_ids) == int(index_gloss))\n",
        "                word_candidate = sequence_output[b_g_idx][word_indexes].mean(dim=0).unsqueeze(0)\n",
        "\n",
        "                mm = torch.mm(word_candidate, cls_embeddings.T)\n",
        "\n",
        "                if labels is not None:\n",
        "\n",
        "                    label_name = labels[b_g_idx][index_gloss][0]\n",
        "\n",
        "                    label_idx = candidates_list[b_g_idx][index_gloss].index(label_name)\n",
        "\n",
        "                    label = torch.tensor([label_idx]).to(hypers.device)\n",
        "\n",
        "                    loss += criterion[label_name](mm, label)\n",
        "                    total_length += 1\n",
        "\n",
        "                    # free RAM and GPU RAM\n",
        "                    label.detach()\n",
        "\n",
        "                    del label_idx\n",
        "                    del label_name\n",
        "                    del label\n",
        "\n",
        "                else:\n",
        "                    prediction_batch.append(candidates_list[b_g_idx][index_gloss][mm.argmax()])\n",
        "\n",
        "                outputs_gloss.detach()\n",
        "                cls_embeddings.detach()\n",
        "                mm.detach()\n",
        "                word_candidate.detach()\n",
        "\n",
        "                del outputs_gloss\n",
        "                del cls_embeddings\n",
        "                del mm\n",
        "                del word_candidate\n",
        "\n",
        "            predictions.append(prediction_batch)\n",
        "\n",
        "\n",
        "\n",
        "        if labels is not None:\n",
        "\n",
        "            loss = loss / total_length\n",
        "\n",
        "            outputs = (loss, outputs)\n",
        "\n",
        "            return outputs  # (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "        else:\n",
        "            return predictions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4YL_LUWxjtO"
      },
      "source": [
        "## Coarse Grained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LxIZVYGzbw7"
      },
      "outputs": [],
      "source": [
        "#Incorporate other staff\n",
        "\n",
        "class GlossWSDGrained(nn.Module):\n",
        "\n",
        "    def __init__(self, hypers):\n",
        "        super(GlossWSDGrained, self).__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "        self.bert_context = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "        self.bert_gloss = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "        self.embedding_lemmas = nn.Embedding(hypers.vocab_size + 1, hypers.embedding_dim, padding_idx = lemmas2idx['PAD'])\n",
        "        self.embedding_pos = nn.Embedding(hypers.vocab_pos_size + 1, hypers.embedding_dim, padding_idx = pos2idx['PAD'])\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hypers.embedding_dim)\n",
        "\n",
        "        #fc layer transforms the output to give the final output layer\n",
        "\n",
        "        # self.fc1 = nn.Linear(hypers.embedding_dim * 3, hypers.embedding_dim * 2)\n",
        "        self.fc1 = nn.Linear(hypers.embedding_dim, hypers.embedding_dim * 2)\n",
        "\n",
        "        self.fc2 = nn.Linear(hypers.embedding_dim * 2, hypers.embedding_dim * 1)\n",
        "\n",
        "        self.classifier = nn.Linear(hypers.embedding_dim, hypers.number_of_tags)\n",
        "\n",
        "        self.dropout = nn.Dropout(hypers.dropout_rate)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.hypers = hypers\n",
        "\n",
        "    def forward(self, batches_tokens, lemmas = None, pos = None, target_mask = None, labels = None, glosses = None):\n",
        "\n",
        "        # lemmas = self.dropout(self.embedding_lemmas(lemmas))   # dim: batch_size x batch_max_len x embedding_dim\n",
        "        # pos = self.dropout(self.embedding_pos(pos))\n",
        "\n",
        "        # batch_size x batch_max_len x embedding_dim ->\n",
        "        # batch_size x batch_max_len x 1(50) x embedding_dim\n",
        "        # batch_size x batch_max_len x 50 x embedding_dim\n",
        "\n",
        "        x = self.tokenizer(batches_tokens, return_tensors=\"pt\",\n",
        "                                  padding='longest',\n",
        "                                  is_split_into_words=True).to(hypers.device)\n",
        "\n",
        "        attention_mask = x['attention_mask'].squeeze(1).to(hypers.device)\n",
        "        input_ids = x['input_ids'].squeeze(1).to(hypers.device)\n",
        "\n",
        "        outputs = self.bert_context(input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        if glosses is not None:\n",
        "            for b_g_idx, b_g in enumerate(glosses): #b_g = batch_gloss\n",
        "                for index_gloss, gloss in b_g.items():\n",
        "                    gloss_arr = [[g] for g in gloss]\n",
        "                    gloss_embed = self.tokenizer(gloss_arr, return_tensors=\"pt\",\n",
        "                                    padding='longest',\n",
        "                                    is_split_into_words=True).to(hypers.device)\n",
        "\n",
        "                    outputs_gloss = self.bert_gloss(**gloss_embed).last_hidden_state #senses X max_len X 768\n",
        "\n",
        "                    cls_embeddings = outputs_gloss[:, 0, :] #senses X 768\n",
        "\n",
        "                    word_candidate = sequence_output[b_g_idx, int(index_gloss) + 1, :].unsqueeze(0).detach().cpu().numpy() # 1 X 768\n",
        "\n",
        "                    dot_prod = np.dot(word_candidate, cls_embeddings.detach().cpu().numpy().T)\n",
        "                    best_sense_idx = dot_prod.argmax()\n",
        "\n",
        "                    #substitute the embedding of a word to disambguate adding to it the embedding of the most similar sense\n",
        "                    sequence_output[b_g_idx, best_sense_idx] = (sequence_output[b_g_idx, best_sense_idx] + cls_embeddings[best_sense_idx,:]) / 2\n",
        "\n",
        "                    outputs_gloss.detach()\n",
        "                    cls_embeddings.detach()\n",
        "\n",
        "                    del outputs_gloss\n",
        "                    del cls_embeddings\n",
        "                    del word_candidate\n",
        "                    del dot_prod\n",
        "                    del best_sense_idx\n",
        "\n",
        "        # concat = torch.cat((sequence_output, lemmas, pos), dim = -1) #batch X words_len X embedding_dim * 3\n",
        "\n",
        "        concat = self.dropout(self.relu(self.fc1(sequence_output))) #batch X words_len X embedding_dim * 2\n",
        "        concat = self.dropout(self.relu(self.fc2(concat))) #batch X words_len X embedding_dim\n",
        "\n",
        "        logits = self.classifier(concat)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, hypers.number_of_tags)[active_loss]\n",
        "                active_labels = labels.view(-1)[active_loss]\n",
        "\n",
        "\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, hypers.number_of_tags), labels.view(-1))\n",
        "\n",
        "\n",
        "\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjIIl4APZ_C-"
      },
      "outputs": [],
      "source": [
        "# # WORKING VERSION\n",
        "\n",
        "class BaselineWSD(nn.Module):\n",
        "\n",
        "    def __init__(self, hypers):\n",
        "        super(BaselineWSD, self).__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "        self.bert = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "        self.embedding_lemmas = nn.Embedding(hypers.vocab_size + 1, hypers.embedding_dim, padding_idx = lemmas2idx['PAD'])\n",
        "        self.embedding_pos = nn.Embedding(hypers.vocab_pos_size + 1, hypers.embedding_dim, padding_idx = pos2idx['PAD'])\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hypers.embedding_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(hypers.embedding_dim * 3, hypers.embedding_dim * 1)\n",
        "        self.fc2 = nn.Linear(hypers.embedding_dim * 2, hypers.embedding_dim * 1) #Kept to match the parameters found in training and saved but it is not used\n",
        "        self.classifier = nn.Linear(hypers.embedding_dim, hypers.number_of_tags)\n",
        "\n",
        "        self.dropout = nn.Dropout(hypers.dropout_rate)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.hypers = hypers\n",
        "\n",
        "    def forward(self, batches_tokens, lemmas, pos, target_mask, labels = None):\n",
        "\n",
        "        lemmas = self.dropout(self.embedding_lemmas(lemmas))   # dim: batch_size x batch_max_len x embedding_dim\n",
        "        pos = self.dropout(self.embedding_pos(pos))\n",
        "\n",
        "        # batch_size x batch_max_len x embedding_dim ->\n",
        "        # batch_size x batch_max_len x 1(50) x embedding_dim\n",
        "        # batch_size x batch_max_len x 50 x embedding_dim\n",
        "\n",
        "        x = tokenizer(batches_tokens, return_tensors=\"pt\",\n",
        "                                  padding='longest',\n",
        "                                  is_split_into_words=True).to(hypers.device)\n",
        "\n",
        "        attention_mask = x['attention_mask'].squeeze(1).to(hypers.device)\n",
        "        input_ids = x['input_ids'].squeeze(1).to(hypers.device)\n",
        "\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        concat = torch.cat((sequence_output, lemmas, pos), dim = -1) #batch X words_len X embedding_dim * 3\n",
        "        concat = self.dropout(self.relu(self.fc1(concat))) #batch X words_len X embedding_dim\n",
        "\n",
        "        logits = self.classifier(concat)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, hypers.number_of_tags)[active_loss]\n",
        "                active_labels = labels.view(-1)[active_loss]\n",
        "\n",
        "\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, hypers.number_of_tags), labels.view(-1))\n",
        "\n",
        "\n",
        "\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOuyP2rGDT4X",
        "tags": []
      },
      "source": [
        "# Train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80n48Eh6ztps"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_5dTGLiTe5t"
      },
      "outputs": [],
      "source": [
        "model_fine = GlossWSDFine(hypers_fine)\n",
        "model_fine.to(hypers_fine.device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_fine.parameters(), lr=hypers_fine.learning_rate)\n",
        "\n",
        "wsd_handler_fine = WSDModuleFine(hypers_fine, model_fine, optimizer, \"drive/MyDrive/AI/nlp_model_hw2_gloss_fine_epoch_2.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4S4DyTCdfVG"
      },
      "outputs": [],
      "source": [
        "it_losses, epoch_losses = wsd_handler_fine.train(train_dataloader, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0dHKrdTDT4y"
      },
      "outputs": [],
      "source": [
        "model = BaselineWSD(hypers)\n",
        "model.to(hypers.device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hypers.learning_rate)\n",
        "\n",
        "wsd_handler_coarse = WSDModuleGrained(hypers, model, optimizer, idx2tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-f7TnLXDT4z"
      },
      "outputs": [],
      "source": [
        "it_losses, epoch_losses = wsd_handler_coarse.train(train_dataloader, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBJQTIBHX-S3"
      },
      "source": [
        "## Aux Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqRGWinmYAGG"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(predictions: List[List[str]], labels: List[List[str]]) -> Tuple[int, int, int]:\n",
        "  '''\n",
        "  Compute the accuracy.\n",
        "  '''\n",
        "  all_accuracies = []\n",
        "  total_accuracy = 0\n",
        "  total_counter = 0\n",
        "\n",
        "  for batch_index in range(len(predictions)):\n",
        "    accuracy = 0\n",
        "    counter = 0\n",
        "    for sen_pred, sen_lab in zip(predictions[batch_index], labels[batch_index]):\n",
        "      for word_pred, word_lab in zip(sen_pred, sen_lab):\n",
        "        counter += 1\n",
        "        total_counter += 1\n",
        "        if word_pred in word_lab:\n",
        "          accuracy += 1\n",
        "          total_accuracy += 1\n",
        "\n",
        "    all_accuracies.append(accuracy / counter)\n",
        "  return all_accuracies, total_accuracy, total_counter\n",
        "\n",
        "def filter_candidate_given_coarse_pred(candidates_list: List[Dict], glosses: List[Dict], pred_coarse: List[List[str]]) -> Tuple[List[Dict], List[Dict]]:\n",
        "  '''\n",
        "  Given the coarse-grained prediction from the coarse-grained model,\n",
        "  filters the possible fine-grained candidates to the just the ones belonging to that cluster.\n",
        "\n",
        "  Return the list of new sense candidates for fine-grained with their relative definitions.\n",
        "\n",
        "  Example of candidate_list:\n",
        "  [{'52': ['knot.n.h.01', 'mil.n.h.03', 'mi.n.h.04', 'mi.n.h.06'],\n",
        "  '69': ['us.n.h.02']},\n",
        "  {'24': ['stay.v.h.02', 'rest.v.h.02', 'stay.v.h.01']}]\n",
        "  '''\n",
        "  new_candidate_list = []\n",
        "  new_glosses = []\n",
        "\n",
        "  for id_sample, sample in enumerate(candidates_list):\n",
        "      new_candidates = {}\n",
        "      new_gloss = {}\n",
        "\n",
        "      for f, (id_candidates, candidates) in enumerate(sample.items()):\n",
        "          new_candidate = []\n",
        "          new_gls = []\n",
        "          # print(candidates)\n",
        "\n",
        "          for i, candidate in enumerate(candidates):\n",
        "\n",
        "              if glossary.map_fine_to_grained[candidate]['cluster'] in pred_coarse[id_sample][f] or pred_coarse[id_sample][f] == 'UNK':\n",
        "                  # print(f'cluster: {pred_coarse[id_sample][f]}, candidate: {candidate}')\n",
        "\n",
        "                  new_candidate.append(candidate)\n",
        "                  new_gls.append(glosses[id_sample][id_candidates][i])\n",
        "\n",
        "          new_candidates[id_candidates] = new_candidate\n",
        "          new_gloss[id_candidates] = new_gls\n",
        "\n",
        "      new_candidate_list.append(new_candidates)\n",
        "      new_glosses.append(new_gloss)\n",
        "\n",
        "  return new_candidate_list, new_glosses\n",
        "\n",
        "def dict_to_arr_lab(labels: List[Dict]) -> List[List[List[str]]]:\n",
        "  '''\n",
        "  Convert a candidate list from List[Dict] to List[List[List[str]]]\n",
        "\n",
        "  Example:\n",
        "  From:\n",
        "  [{'52': ['knot.n.h.01', 'mil.n.h.03', 'mi.n.h.04', 'mi.n.h.06'],\n",
        "  '69': ['us.n.h.02']},\n",
        "  {'24': ['stay.v.h.02', 'rest.v.h.02', 'stay.v.h.01']}]\n",
        "\n",
        "  To:\n",
        "  [[['knot.n.h.01', 'mil.n.h.03', 'mi.n.h.04', 'mi.n.h.06'], ['us.n.h.02']],\n",
        "  [['stay.v.h.02', 'rest.v.h.02', 'stay.v.h.01']]]\n",
        "  '''\n",
        "\n",
        "  labels_arr = [ [word[0] for word in list(sample.values())] for sample in labels]\n",
        "  return labels_arr\n",
        "\n",
        "def retrieve_cluster_given_senses(predictions: List[List[str]]) -> List[List[str]]:\n",
        "  '''\n",
        "  Given the senses predicted by the fine-grained return the cluster to which they belong.\n",
        "  '''\n",
        "  predictions_grained = []\n",
        "  for pred_batch in predictions:\n",
        "      predictions_batch_grained = []\n",
        "      for p in pred_batch:\n",
        "          predictions_batch_grained.append(glossary.map_fine_to_grained[p]['cluster'])\n",
        "\n",
        "      predictions_grained.append(predictions_batch_grained)\n",
        "\n",
        "  return predictions_grained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igeb38MBZe9P"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9GYKyrSncmV"
      },
      "outputs": [],
      "source": [
        "criterion = {}\n",
        "for key in t_set.map_fine_to_grained.keys():\n",
        "    criterion[key] = torch.nn.CrossEntropyLoss(reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4Y065GbDo6V"
      },
      "outputs": [],
      "source": [
        "# model_bert_context = AutoModel.from_pretrained(\"bert-base-cased\").to(hypers.device)\n",
        "# model_bert_gloss = AutoModel.from_pretrained(\"bert-base-cased\").to(hypers.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMaMpVV90JeJ"
      },
      "source": [
        "### Test on fine-grained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15v8szV0yyUs"
      },
      "outputs": [],
      "source": [
        "# model = GlossWSD(hypers)\n",
        "model_fine = GlossWSDFine(hypers_fine)\n",
        "model_fine.to(hypers_fine.device)\n",
        "\n",
        "model_fine.load_state_dict(torch.load(\"drive/MyDrive/AI/nlp_model_hw2_gloss_fine_epoch_2.pth\", map_location=torch.device(hypers.device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-oEXYNuUHqm"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_fine.eval()\n",
        "\n",
        "dataiter_fine = iter(train_dataloader_fine)\n",
        "# dataiter_coarse = iter(train_dataloader)\n",
        "\n",
        "\n",
        "all_predictions_fine = []\n",
        "all_labels_fine = []\n",
        "\n",
        "for index_batch in tqdm(range(len(train_dataloader_fine)), total = len(train_dataloader_fine)):\n",
        "  data_fine = next(dataiter_fine)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      batches_tokens, candidates_list, glosses, labels = data_fine\n",
        "      predictions = model_fine(batches_tokens, glosses, criterion, candidates_list)\n",
        "\n",
        "      all_predictions_fine.append(predictions)\n",
        "      all_labels_fine.append(labels)\n",
        "\n",
        "      # if index_batch == 1:\n",
        "      #   break\n",
        "\n",
        "all_labels_fine_arr = [dict_to_arr_lab(l) for l in all_labels_fine]\n",
        "\n",
        "accuracies, total_acc, total_counter = compute_accuracy(all_predictions_fine, all_labels_fine_arr)\n",
        "print(f'accuracy: {total_acc/total_counter}, loss: {1-(total_acc/total_counter)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w599QlOIIRFX"
      },
      "source": [
        "### Test on corase_grained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4tZNQEJvfCb"
      },
      "outputs": [],
      "source": [
        "model = GlossWSDGrained(hypers)\n",
        "# model = BaselineWSD(hypers)\n",
        "model.to(hypers.device)\n",
        "model.load_state_dict(torch.load(\"drive/MyDrive/AI/nlp_model_hw2_gloss_coarse_epoch_7.pth\", map_location=torch.device(hypers.device)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDYs2KtLIRFe"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "dataiter = iter(test_dataloader)\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "for i in tqdm(range(len(test_dataloader)), total = len(test_dataloader)):\n",
        "  data = next(dataiter)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      batches_tokens, batches_lemmas, batches_labels, batches_pos, target_mask, candidate_mask, candidates_list, glosses = data\n",
        "\n",
        "      batches_lemmas = batches_lemmas.to(hypers.device)\n",
        "      batches_labels = batches_labels.to(hypers.device)\n",
        "      batches_pos = batches_pos.to(hypers.device)\n",
        "      target_mask = target_mask.to(hypers.device)\n",
        "      candidate_mask = candidate_mask.to(hypers.device)\n",
        "\n",
        "      #add UNK as possible result\n",
        "      il = batches_labels.nonzero(as_tuple=True) + (torch.tensor([1]), )\n",
        "      candidate_mask[il] = 1\n",
        "\n",
        "      loss, output = model(batches_tokens, batches_lemmas, batches_pos, target_mask, batches_labels)\n",
        "      # output = model(batches_tokens, batches_lemmas, batches_pos, target_mask)\n",
        "\n",
        "      pred_coarse, labels_coarse = wsd_handler_coarse.getPredictionLabels(output, candidate_mask, batches_labels)\n",
        "\n",
        "      all_predictions.append(pred_coarse)\n",
        "      all_labels.append(labels_coarse)\n",
        "\n",
        "\n",
        "accuracies, total_acc, total_counter = compute_accuracy(all_predictions, all_labels)\n",
        "print(f'accuracy: {total_acc/total_counter}, loss: {1-(total_acc/total_counter)}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEvi3bOudMKZ"
      },
      "source": [
        "### Test before coarse, then fine(prediction on fine dataset)\n",
        "I use the corase grained model to predict the correct cluster from which i extract all the sense candidates. On these candidates i disambiguate the correct sense with the model trained on the fine-grained dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWde-CpOdMKl",
        "outputId": "6c8f5d09-5809-443b-8ed2-1041ebc5b858"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [00:16<00:00,  4.80it/s]\n"
          ]
        }
      ],
      "source": [
        "dataiter_fine = iter(test_dataloader_fine)\n",
        "dataiter_coarse = iter(test_dataloader)\n",
        "\n",
        "\n",
        "all_predictions_fine = []\n",
        "all_labels_fine = []\n",
        "\n",
        "all_predictions_coarse = []\n",
        "all_labels_coarse = []\n",
        "\n",
        "for index_batch in tqdm(range(len(test_dataloader_fine)), total = len(test_dataloader_fine)):\n",
        "  data_fine = next(dataiter_fine)\n",
        "  data_coarse = next(dataiter_coarse)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      batches_tokens, batches_lemmas, batches_labels, batches_pos, target_mask, candidate_mask, candidates_list, glosses = data_coarse\n",
        "\n",
        "      batches_lemmas = batches_lemmas.to(hypers.device)\n",
        "      batches_labels = batches_labels.to(hypers.device)\n",
        "      batches_pos = batches_pos.to(hypers.device)\n",
        "      target_mask = target_mask.to(hypers.device)\n",
        "      candidate_mask = candidate_mask.to(hypers.device)\n",
        "\n",
        "      il = batches_labels.nonzero(as_tuple=True) + (torch.tensor([1]), )\n",
        "      candidate_mask[il] = 1\n",
        "\n",
        "      # predictions = model(batches_tokens, batches_lemmas, batches_pos, target_mask, batches_labels, glosses, criterion, candidates_list)\n",
        "      loss, output = model(batches_tokens, batches_lemmas, batches_pos, target_mask, batches_labels)\n",
        "      pred_coarse, labels_coarse = wsd_handler_coarse.getPredictionLabels(output, candidate_mask, batches_labels)\n",
        "\n",
        "      all_predictions_coarse.append(pred_coarse)\n",
        "      all_labels_coarse.append(labels_coarse)\n",
        "\n",
        "      batches_tokens, candidates_list, glosses, labels = data_fine\n",
        "\n",
        "      new_candidates_list, new_glosses = filter_candidate_given_coarse_pred(candidates_list, glosses, pred_coarse)\n",
        "\n",
        "      predictions = model_fine(batches_tokens, new_glosses, criterion, new_candidates_list)\n",
        "\n",
        "      all_predictions_fine.append(predictions)\n",
        "      all_labels_fine.append(labels)\n",
        "\n",
        "      # if index_batch == 1:\n",
        "      #   break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQb1oDi7lI8L",
        "outputId": "47a30082-43a5-4385-a41e-ba09f6571606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracies mean: 0.7999563037760133\n",
            "accuracy: 0.801025641025641\n",
            "accuracies mean: 0.9194186861012681\n",
            "accuracy: 0.918974358974359\n"
          ]
        }
      ],
      "source": [
        "all_labels_fine_arr = [dict_to_arr_lab(l) for l in all_labels_fine]\n",
        "\n",
        "accuracies, total_acc, total_counter = compute_accuracy(all_predictions_fine, all_labels_fine_arr)\n",
        "print(f'accuracy: {total_acc/total_counter}')\n",
        "\n",
        "accuracies, total_acc, total_counter = compute_accuracy(all_predictions_coarse, all_labels_coarse)\n",
        "print(f'accuracy: {total_acc/total_counter}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sI2spCNcQCs"
      },
      "source": [
        "### Test Before fine, then extract the coarse cluster(prediction on coarse dataset)\n",
        "\n",
        "Here i predict with the model trained in the fine-grained and then extract the cluster to chich the senses belongs to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-gZoFw4Uv2T"
      },
      "outputs": [],
      "source": [
        "dataiter_fine = iter(test_dataloader_fine)\n",
        "dataiter_coarse = iter(test_dataloader)\n",
        "\n",
        "\n",
        "all_predictions_fine = []\n",
        "all_labels_fine = []\n",
        "\n",
        "all_labels_coarse = []\n",
        "\n",
        "\n",
        "for index_batch in tqdm(range(len(test_dataloader_fine)), total = len(test_dataloader_fine)):\n",
        "  data_fine = next(dataiter_fine)\n",
        "  data_coarse = next(dataiter_coarse)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      batches_tokens, candidates_list, glosses, labels = data_fine\n",
        "      predictions = model_fine(batches_tokens, glosses, criterion, candidates_list)\n",
        "\n",
        "      all_predictions_fine.append(predictions)\n",
        "      all_labels_fine.append(labels)\n",
        "\n",
        "      batches_tokens, batches_lemmas, batches_labels, batches_pos, target_mask, candidate_mask, candidates_list, glosses = data_coarse\n",
        "\n",
        "      cluster_labels = [[] for i in range(len(batches_tokens))]\n",
        "      cluster_labels_indices = batches_labels.nonzero(as_tuple=True)\n",
        "      labels = batches_labels[cluster_labels_indices].tolist()\n",
        "\n",
        "\n",
        "      for i, elem in enumerate(cluster_labels_indices[0]):\n",
        "        cluster_labels[elem.item()].append(idx2tags[str(labels[i])])\n",
        "\n",
        "      all_labels_coarse.append(cluster_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DQAHK4MV6oN"
      },
      "outputs": [],
      "source": [
        "all_predictions_fine_arr = [elem for batches in all_predictions_fine for elem in batches ]\n",
        "all_labels_fine_arr = [elem for batches in all_labels_fine for elem in batches ]\n",
        "all_labels_coarse_arr = [elem for batches in all_labels_coarse for elem in batches ]\n",
        "labels_arr = dict_to_arr_lab(all_labels_fine_arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8ST6FFRrU9G"
      },
      "outputs": [],
      "source": [
        "# print(compute_accuracy([all_predictions_fine_arr], [labels_arr]))\n",
        "print(compute_accuracy([retrieve_cluster_given_senses(all_predictions_fine_arr)], [retrieve_cluster_given_senses(labels_arr)]))"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "collapsed_sections": [
        "jNtbaQHcWDz5",
        "j5TXd1-PmeIF",
        "B4UninTAG-3l",
        "cFAl2R27G-3l",
        "6_BRG8T5G-3m",
        "FdOcwzboLZ1t",
        "tSF_qCfmLetL",
        "-CxtI8RpG-3o",
        "9IRyFS7XYHOy",
        "aNcCo3-bYJcq",
        "X0gjCvJ0zHRd",
        "OPePshLRuW5H",
        "Exsqg77-xhGJ",
        "y4YL_LUWxjtO",
        "hOuyP2rGDT4X",
        "80n48Eh6ztps",
        "OBJQTIBHX-S3",
        "Igeb38MBZe9P",
        "KMaMpVV90JeJ",
        "w599QlOIIRFX",
        "cEvi3bOudMKZ",
        "4sI2spCNcQCs"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}